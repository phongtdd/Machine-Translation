{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10167921,"sourceType":"datasetVersion","datasetId":6279240}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORT","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom transformers import  AutoTokenizer, AutoModel\nfrom tokenizers import Tokenizer\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, MultiHeadAttention, LayerNormalization, Add, Dense, Dropout\n\nimport tensorflow as tf\nimport numpy as np\nimport random\nnltk.download('punkt')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA PROCESS","metadata":{}},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"vi_sentences_path = \"/kaggle/input/berttokenize/Bert/tokenize_vi.txt\" # change at will\nen_sentences_path = \"/kaggle/input/berttokenize/Bert/tokenize_en.txt\" # change at will\ntokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntokenizer_vi = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vi_vocab_size = tokenizer_vi.vocab_size\nen_vocab_size = tokenizer_en.vocab_size\n\nprint(f\"Vietnamese Vocabulary Size: {vi_vocab_size}\")\nprint(f\"English Vocabulary Size: {en_vocab_size}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_text = \"This is an English sentence\"\n\ninput_ids = tokenizer_en.encode(input_text, return_tensors=\"pt\")\n\nprint(input_ids)\n\ndecoded_text = tokenizer_en.decode(input_ids[0], skip_special_tokens=True)\n\nprint(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_text = \"Với bài toán dịch Anh - Việt, việc kiểm tra cách mà tokenizer mã hóa câu tiếng Anh và tái mã hóa lại câu tiếng Việt là rất quan trọng. Dưới đây là hướng dẫn cụ thể\"\n\n# Tokenize câu\ninput_ids = tokenizer_vi.encode(input_text)\n\nprint(input_ids)\n\ndecoded_text = tokenizer_vi.decode(input_ids, skip_special_tokens=True)\n\nprint(\"Decoded Text:\", decoded_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split data","metadata":{}},{"cell_type":"code","source":"def count_sentences(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n    return len(lines)\n\n# Example: Count the number of sentences in the tokenized_vi.txt file\n\nnum_sentences_vi = count_sentences(vi_sentences_path)\nnum_sentences_en = count_sentences(en_sentences_path)\n\nprint(f\"Number of sentences in tokenized vietnamese: {num_sentences_vi}\")\nprint(f\"Number of sentences in tokenized english: {num_sentences_en}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retrieve the vocabulary for both tokenizers\nvi_vocab = tokenizer_vi.get_vocab()  \nen_vocab = tokenizer_en.get_vocab()  \n\n# Print the first 20 tokens from the English vocabulary\nprint(\"First 20 tokens in the English vocabulary:\")\nfor i, (token, _) in enumerate(list(en_vocab.items())[:20]):\n    print(f\"{i+1}. {token}\")\n\n# Print the first 20 tokens from the Vietnamese vocabulary\nprint(\"\\nFirst 20 tokens in the Vietnamese vocabulary:\")\nfor i, (token, _) in enumerate(list(vi_vocab.items())[:20]):\n    print(f\"{i+1}. {token}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read tokenized sentences\ndef read_tokenized_sentences(file_path):\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        lines = file.readlines()\n    return [list(map(int, line.strip().split())) for line in lines]\n\n# Load tokenized sentences\ntokenized_en = read_tokenized_sentences(en_sentences_path)\ntokenized_vi = read_tokenized_sentences(vi_sentences_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Take only 1% of the data\ndef sample_data(english_sentences, vietnamese_sentences, sample_ratio=0.02):\n    dataset_size = len(english_sentences)\n    sample_size = int(sample_ratio * dataset_size)\n    indices = np.random.choice(dataset_size, sample_size, replace=False)\n\n    sampled_en = [english_sentences[i] for i in indices]\n    sampled_vi = [vietnamese_sentences[i] for i in indices]\n\n    return sampled_en, sampled_vi\n\nsampled_en, sampled_vi = sample_data(tokenized_en, tokenized_vi,sample_ratio=0.02)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Dataset","metadata":{}},{"cell_type":"code","source":"def create_tf_dataset(english_sentences, vietnamese_sentences, train_split=0.9):\n    dataset_size = len(english_sentences)\n    indices = np.arange(dataset_size)\n    np.random.shuffle(indices)\n\n    train_size = int(train_split * dataset_size)\n    train_indices = indices[:train_size]\n    val_indices = indices[train_size:]\n\n    def select_data(indices):\n        en_data = tf.constant([english_sentences[i] for i in indices], dtype=tf.int32)\n        vi_data = tf.constant([vietnamese_sentences[i] for i in indices], dtype=tf.int32)\n        return tf.data.Dataset.from_tensor_slices((en_data, vi_data))\n\n    train_data = select_data(train_indices)\n    val_data = select_data(val_indices)\n\n    return train_data, val_data\n    \ntrain_data, val_data = create_tf_dataset(sampled_en, sampled_vi, train_split=0.9)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_data_dynamic_parallel(dataset):\n    def map_func(english, vietnamese):\n        # Split Vietnamese target into input (targ_in) and output (targ_out)\n        targ_in = vietnamese[:, :-1]\n        targ_out = vietnamese[:, 1:]\n        return (tf.cast(english, tf.int64), tf.cast(targ_in, tf.int64)), tf.cast(targ_out, tf.int64)\n\n    return (\n        dataset.shuffle(10000)\n        .batch(BATCH_SIZE, drop_remainder=False)\n        .map(map_func, num_parallel_calls=tf.data.AUTOTUNE)  # Parallel mapping\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare the datasets\nBATCH_SIZE = 32\ntrain_dataset = prepare_data_dynamic_parallel(train_data)\nval_dataset = prepare_data_dynamic_parallel(val_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of how the shapes should look now\nfor (en_batch, targ_in), targ_out in train_dataset.take(1):\n    print(\"English Batch Shape:\", en_batch.shape)\n    print(\"Vietnamese Input Batch Shape:\", targ_in.shape)\n    print(\"Vietnamese Output Batch Shape:\", targ_out.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the dtype\ntrain_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODEL","metadata":{}},{"cell_type":"code","source":"# import wandb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# wandb.login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# wandb.init(project=\"translation-model\", name=\"en-to-vi-translation\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, MultiHeadAttention, LayerNormalization, Add, Dense, Dropout\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model parameters\nVOCAB_SIZE = 64000  \nUNITS = 256\nMAX_LENGTH = 50","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ENCODER LAYER\n\nencoder_input = tf.keras.Input(shape=(None,), dtype=tf.int64)\nembedding = Embedding(input_dim=VOCAB_SIZE, output_dim=UNITS, mask_zero=False)(encoder_input)\nencoder_embedding_dropout = Dropout(0.2)(embedding) \nrnn_output = Bidirectional(LSTM(units=UNITS, return_sequences=True))(encoder_embedding_dropout)\nencoder_output = Dense(UNITS)(rnn_output)  \n\n# CROSS-ATTENTION LAYER\n\ndecoder_input = tf.keras.Input(shape=(None,), dtype=tf.int64)\ndecoder_embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=UNITS, mask_zero=False)(decoder_input)\ndecoder_embedding_dropout = Dropout(0.2)(decoder_embedding)\npre_attention_rnn = LSTM(units=UNITS, return_sequences=True, return_state=True)(decoder_embedding_dropout)\nattn_output = MultiHeadAttention(key_dim=UNITS, num_heads=4)(query=pre_attention_rnn[0], value=encoder_output)\nattn_output = Add()([pre_attention_rnn[0], attn_output])\nattn_output = LayerNormalization()(attn_output)\n\n# DECODER LAYER\npost_attention_rnn = LSTM(units=UNITS, return_sequences=True)(attn_output)\nlogits = Dense(VOCAB_SIZE, activation='softmax')(post_attention_rnn)\n\n# FINAL MODEL\nmodel = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=logits)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\ndef compile_and_train(model, epochs=40, steps_per_epoch=3200):\n    \n    early_stopping = EarlyStopping(\n        monitor='val_loss',  \n        patience=4,          \n        restore_best_weights=True  \n    )\n\n    model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n\n    history = model.fit(\n        train_dataset.repeat(),\n        epochs=epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_dataset,\n        validation_steps=350,\n        callbacks=[early_stopping]  \n    )\n\n    return model, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"# Training\ntrained_translator, history = compile_and_train(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_translator.save('/kaggle/working/best_model.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = load_model('/kaggle/working/best_model.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# INFERENCE","metadata":{}},{"cell_type":"code","source":"def greedy_decode(input_sequence, model, tokenizer_target, max_length=50):\n\n    input_sequence = tf.constant([input_sequence], dtype=tf.int64)\n\n    start_token = tokenizer_vi.cls_token_id\n    end_token = tokenizer_vi.sep_token_id\n\n    target_sequence = [start_token]\n\n    for _ in range(max_length):\n        decoder_input = tf.constant([target_sequence], dtype=tf.int64)\n\n        predictions = model.predict([input_sequence, decoder_input], verbose=0)\n\n        next_token = tf.argmax(predictions[:, -1, :], axis=-1).numpy()[0]\n\n        target_sequence.append(next_token)\n\n        if next_token == end_token:\n            break\n\n    translated_sentence = tokenizer_target.decode(target_sequence[1:], skip_special_tokens=True)\n    return translated_sentence","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"en_sentence = \"I go to school\"\n\ninput_tokens = tokenizer_en.encode(en_sentence, add_special_tokens=True)\n\ntranslated_sentence = greedy_decode(input_tokens, model, tokenizer_vi)\n\nprint(\"Input Sentence:\", en_sentence)\nprint(\"Translated Sentence:\", translated_sentence)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}