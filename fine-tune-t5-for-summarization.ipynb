{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566},{"sourceId":197071,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":168061,"modelId":190399}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets evaluate accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:42:39.556606Z","iopub.execute_input":"2024-12-12T22:42:39.556970Z","iopub.status.idle":"2024-12-12T22:42:50.977148Z","shell.execute_reply.started":"2024-12-12T22:42:39.556936Z","shell.execute_reply":"2024-12-12T22:42:50.976209Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nroot = '/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:42:50.979894Z","iopub.execute_input":"2024-12-12T22:42:50.980819Z","iopub.status.idle":"2024-12-12T22:42:52.333228Z","shell.execute_reply.started":"2024-12-12T22:42:50.980773Z","shell.execute_reply":"2024-12-12T22:42:52.332459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:42:52.334453Z","iopub.execute_input":"2024-12-12T22:42:52.335050Z","iopub.status.idle":"2024-12-12T22:42:52.339603Z","shell.execute_reply.started":"2024-12-12T22:42:52.334986Z","shell.execute_reply":"2024-12-12T22:42:52.338560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_files = {}\nfor file in os.listdir(root):\n    file_path = root+file\n    name = file.split('.')[0]\n    data_files[name] = os.path.join(root, file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:42:52.340758Z","iopub.execute_input":"2024-12-12T22:42:52.341098Z","iopub.status.idle":"2024-12-12T22:42:52.357412Z","shell.execute_reply.started":"2024-12-12T22:42:52.341063Z","shell.execute_reply":"2024-12-12T22:42:52.356498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:42:52.359471Z","iopub.execute_input":"2024-12-12T22:42:52.359756Z","iopub.status.idle":"2024-12-12T22:42:52.366263Z","shell.execute_reply.started":"2024-12-12T22:42:52.359728Z","shell.execute_reply":"2024-12-12T22:42:52.365356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasets = load_dataset(\"csv\", data_files = data_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:42:52.367570Z","iopub.execute_input":"2024-12-12T22:42:52.367944Z","iopub.status.idle":"2024-12-12T22:43:26.878656Z","shell.execute_reply.started":"2024-12-12T22:42:52.367904Z","shell.execute_reply":"2024-12-12T22:43:26.877580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasets['train'] = datasets['train'].shuffle(seed=42).select(range(30000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:43:26.880019Z","iopub.execute_input":"2024-12-12T22:43:26.880651Z","iopub.status.idle":"2024-12-12T22:43:26.960818Z","shell.execute_reply.started":"2024-12-12T22:43:26.880605Z","shell.execute_reply":"2024-12-12T22:43:26.960011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:43:26.961950Z","iopub.execute_input":"2024-12-12T22:43:26.962270Z","iopub.status.idle":"2024-12-12T22:43:27.883607Z","shell.execute_reply.started":"2024-12-12T22:43:26.962241Z","shell.execute_reply":"2024-12-12T22:43:27.882574Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:43:27.884856Z","iopub.execute_input":"2024-12-12T22:43:27.885203Z","iopub.status.idle":"2024-12-12T22:43:32.956837Z","shell.execute_reply.started":"2024-12-12T22:43:27.885172Z","shell.execute_reply":"2024-12-12T22:43:32.955789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prefix = \"summarize: \"\n\n\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"article\"]]\n    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples[\"highlights\"], max_length=128, truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:43:32.958126Z","iopub.execute_input":"2024-12-12T22:43:32.958605Z","iopub.status.idle":"2024-12-12T22:43:32.964421Z","shell.execute_reply.started":"2024-12-12T22:43:32.958574Z","shell.execute_reply":"2024-12-12T22:43:32.963315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size=256\n\ntokenizer_datasets = datasets.map(preprocess_function, batched = True, batch_size=batch_size, remove_columns=['id', 'article', 'highlights'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:43:32.965494Z","iopub.execute_input":"2024-12-12T22:43:32.965873Z","iopub.status.idle":"2024-12-12T22:45:16.108174Z","shell.execute_reply.started":"2024-12-12T22:43:32.965826Z","shell.execute_reply":"2024-12-12T22:45:16.107121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:45:16.109314Z","iopub.execute_input":"2024-12-12T22:45:16.109607Z","iopub.status.idle":"2024-12-12T22:45:16.116314Z","shell.execute_reply.started":"2024-12-12T22:45:16.109579Z","shell.execute_reply":"2024-12-12T22:45:16.115109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:45:16.117715Z","iopub.execute_input":"2024-12-12T22:45:16.118116Z","iopub.status.idle":"2024-12-12T22:45:33.760878Z","shell.execute_reply.started":"2024-12-12T22:45:16.118074Z","shell.execute_reply":"2024-12-12T22:45:33.759883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:45:33.767218Z","iopub.execute_input":"2024-12-12T22:45:33.768430Z","iopub.status.idle":"2024-12-12T22:45:33.773501Z","shell.execute_reply.started":"2024-12-12T22:45:33.768381Z","shell.execute_reply":"2024-12-12T22:45:33.772461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=15,\n    fp16=True,\n    report_to = 'none'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:45:33.774728Z","iopub.execute_input":"2024-12-12T22:45:33.775011Z","iopub.status.idle":"2024-12-12T22:45:33.935340Z","shell.execute_reply.started":"2024-12-12T22:45:33.774983Z","shell.execute_reply":"2024-12-12T22:45:33.934273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenizer_datasets[\"train\"],\n    eval_dataset=tokenizer_datasets[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:45:33.936509Z","iopub.execute_input":"2024-12-12T22:45:33.936792Z","iopub.status.idle":"2024-12-12T23:16:41.325442Z","shell.execute_reply.started":"2024-12-12T22:45:33.936764Z","shell.execute_reply":"2024-12-12T23:16:41.324474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained('pretrained_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T23:19:43.555668Z","iopub.execute_input":"2024-12-12T23:19:43.556382Z","iopub.status.idle":"2024-12-12T23:19:44.166893Z","shell.execute_reply.started":"2024-12-12T23:19:43.556345Z","shell.execute_reply":"2024-12-12T23:19:44.165901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Load the tokenizer\ntokenizer = T5Tokenizer.from_pretrained('/kaggle/input/21312/keras/default/1/results/checkpoint-28125')\n\n# Load the model\nmodel = T5ForConditionalGeneration.from_pretrained('/kaggle/input/21312/keras/default/1/results/checkpoint-28125')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T09:54:37.088049Z","iopub.execute_input":"2024-12-13T09:54:37.088932Z","iopub.status.idle":"2024-12-13T09:54:38.076533Z","shell.execute_reply.started":"2024-12-13T09:54:37.088894Z","shell.execute_reply":"2024-12-13T09:54:38.075845Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"input_text = \"summarize: The quick brown fox jumps over the lazy dog. This text is part of an example to demonstrate how summarization works using T5.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T10:10:43.205088Z","iopub.execute_input":"2024-12-13T10:10:43.205545Z","iopub.status.idle":"2024-12-13T10:10:43.210199Z","shell.execute_reply.started":"2024-12-13T10:10:43.205502Z","shell.execute_reply":"2024-12-13T10:10:43.209294Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).input_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T10:10:51.746863Z","iopub.execute_input":"2024-12-13T10:10:51.747600Z","iopub.status.idle":"2024-12-13T10:10:51.752228Z","shell.execute_reply.started":"2024-12-13T10:10:51.747565Z","shell.execute_reply":"2024-12-13T10:10:51.751314Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Load fine-tuned model and tokenizer\n# Load the tokenizer\ntokenizer = T5Tokenizer.from_pretrained('/kaggle/input/21312/keras/default/1/results/checkpoint-28125')\n\n# Load the model\nmodel = T5ForConditionalGeneration.from_pretrained('/kaggle/input/21312/keras/default/1/results/checkpoint-28125')\n\n\n# Text to summarize\ninput_text = (\n    \"summarize: The Transformers library provides thousands of pretrained models to perform tasks on texts such as classification, \"\n    \"information extraction, question answering, summarization, translation, text generation, and more. It is maintained by Hugging Face.\"\n)\n\n# Tokenize input\ninput_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).input_ids\n\n# Generate summary\noutputs = model.generate(input_ids, max_length=100, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)\n\n# Decode and print summary\nsummary = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Summary:\", summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T10:11:18.057121Z","iopub.execute_input":"2024-12-13T10:11:18.057496Z","iopub.status.idle":"2024-12-13T10:11:21.779202Z","shell.execute_reply.started":"2024-12-13T10:11:18.057465Z","shell.execute_reply":"2024-12-13T10:11:21.778321Z"}},"outputs":[{"name":"stdout","text":"Summary: Transformers library provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation.\n","output_type":"stream"}],"execution_count":7}]}