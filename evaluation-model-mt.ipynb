{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:21:43.276260Z",
     "iopub.status.busy": "2024-12-15T04:21:43.275305Z",
     "iopub.status.idle": "2024-12-15T04:21:43.319617Z",
     "shell.execute_reply": "2024-12-15T04:21:43.318800Z",
     "shell.execute_reply.started": "2024-12-15T04:21:43.276209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "root = \"/kaggle/input/\"   # root path of dataset\n",
    "def read_data(root, folder):\n",
    "    data = {'en':[], 'vi':[]}\n",
    "    path = os.path.join(root, folder)\n",
    "    for file_name in os.listdir(path):\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        with open(file_path,'r') as f:\n",
    "            _, tail = file_path.split('.')\n",
    "            if tail =='en':\n",
    "                for line in f:\n",
    "                    data['en'].append(line.strip())\n",
    "            else:\n",
    "                for line in f:\n",
    "                    data['vi'].append(line.strip())\n",
    "                    \n",
    "    return data\n",
    "\n",
    "test_data = read_data(root, 'datatest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:21:45.563204Z",
     "iopub.status.busy": "2024-12-15T04:21:45.562845Z",
     "iopub.status.idle": "2024-12-15T04:21:46.767869Z",
     "shell.execute_reply": "2024-12-15T04:21:46.766946Z",
     "shell.execute_reply.started": "2024-12-15T04:21:45.563172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_dict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:21:46.769715Z",
     "iopub.status.busy": "2024-12-15T04:21:46.769371Z",
     "iopub.status.idle": "2024-12-15T04:21:46.776282Z",
     "shell.execute_reply": "2024-12-15T04:21:46.775247Z",
     "shell.execute_reply.started": "2024-12-15T04:21:46.769678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'vi'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderDecoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model_Bert_BARTPho(model_path):\n",
    "    encoder_tokenizer = \"bert-base-uncased\"  \n",
    "    decoder_tokenizer = \"vinai/bartpho-word\"  \n",
    "    \n",
    "    encoder = AutoTokenizer.from_pretrained(encoder_tokenizer)\n",
    "    decoder = AutoTokenizer.from_pretrained(decoder_tokenizer)\n",
    "    model = EncoderDecoderModel.from_pretrained(model_path)  # Load model pretrain\n",
    "    return encoder, decoder, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_predict_EncoderDecoder(model_path, test_data):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder, decoder, model = load_model_Bert_BARTPho(model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    texts = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for item in test_data:\n",
    "        source = item[\"en\"]\n",
    "        target = item[\"vi\"]\n",
    "        \n",
    "        inputs = encoder(source, \n",
    "                        padding=True, \n",
    "                        truncation=True, \n",
    "                        max_length=len(target.split()),\n",
    "                        return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "            # Generate translation\n",
    "        outputs = model.generate(inputs[\"input_ids\"], max_length=64, num_beams=4)\n",
    "        prediction = decoder.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        texts.append(source)\n",
    "        predictions.append(prediction)\n",
    "        references.append(target)\n",
    "        \n",
    "    \n",
    "    return texts, predictions, references    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model_T5(model_path):\n",
    "    encoder = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    decoder = spm.SentencePieceProcessor(model_file = '/kaggle/working/models--Sag1012--machine-translation/snapshots/164bfec8e7d09d77ab222a6055293e66934994ca/T5/vi_tokenizer_32128.model')\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return encoder, decoder, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_predict_T5(model_path, test_data):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder, decoder, model = load_model_T5(model_path)\n",
    "    model = model.to(device)\n",
    "\n",
    "    texts = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for item in test_data:\n",
    "        source = item[\"en\"]\n",
    "        target = item[\"vi\"]\n",
    "        \n",
    "        inputs = encoder(source, \n",
    "                        padding=True, \n",
    "                        truncation=True, \n",
    "                        max_length=len(target.split()),\n",
    "                        return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "        outputs = model.generate(inputs[\"input_ids\"], max_length=64, num_beams=4)\n",
    "        prediction = decoder.decode(outputs[0].tolist())\n",
    "            \n",
    "        texts.append(source)\n",
    "        predictions.append(prediction)\n",
    "        references.append(target)\n",
    "        \n",
    "    \n",
    "    return texts, predictions, references    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_predict_BiLSTM(model_path,test_data):\n",
    "    import os\n",
    "    directory_path = os.path.dirname(model_path)\n",
    "    texts = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "    model = load_model(model_path)\n",
    "    encoder_input = model.input[0]  \n",
    "    encoder_output = model.get_layer(\"bidirectional\").output[0]\n",
    "    encoder_state_h = model.get_layer(\"state_h_concat\").output\n",
    "    encoder_state_c = model.get_layer(\"state_c_concat\").output\n",
    "    \n",
    "    # Encoder inference model\n",
    "    encoder_model = Model(encoder_input, [encoder_output, encoder_state_h, encoder_state_c])\n",
    "    decoder_embedding = model.get_layer(\"decoder_embedding\")\n",
    "    decoder_lstm = model.get_layer(\"decoder_lstm\")\n",
    "    decoder_dense = model.get_layer(\"decoder_dense\")\n",
    "    units = 128  # LSTM units\n",
    "    # Decoder inference inputs\n",
    "    decoder_state_input_h = Input(shape=(units * 2,), name=\"decoder_state_input_h\")  \n",
    "    decoder_state_input_c = Input(shape=(units * 2,), name=\"decoder_state_input_c\")\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    # Reuse the embedding and LSTM layers\n",
    "    decoder_input = Input(shape=(1,), name=\"decoder_input\")  # Decoder input for one time step\n",
    "    decoder_embedding_inf = decoder_embedding(decoder_input)\n",
    "    decoder_lstm_inf = decoder_lstm(decoder_embedding_inf, initial_state=decoder_states_inputs)\n",
    "    decoder_output_inf, state_h_inf, state_c_inf = decoder_lstm_inf\n",
    "    \n",
    "    decoder_states_inf = [state_h_inf, state_c_inf]\n",
    "    \n",
    "    # Dense layer for probabilities\n",
    "    decoder_output_inf = decoder_dense(decoder_output_inf)\n",
    "    \n",
    "    # Decoder inference model\n",
    "    decoder_model = Model(\n",
    "        [decoder_input] + decoder_states_inputs,  # Inputs\n",
    "        [decoder_output_inf] + decoder_states_inf)  # Outputs\n",
    "    def preprocess_sentence(sentence, tokenizer, max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([sentence])\n",
    "        return pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    \n",
    "    def decode_sequence(input_seq):\n",
    "        # Encode the input sequence to get initial states\n",
    "    \n",
    "        encoder_output, state_h, state_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "        target_seq = np.zeros((1, 1))  # Shape: (batch_size, 1)\n",
    "        target_seq[0, 0] = vi_loaded_tokenizer.texts_to_sequences([\"<SOS>\"])[0][0]\n",
    "    \n",
    "        states = [state_h, state_c]\n",
    "    \n",
    "        decoded_sentence = []\n",
    "        for _ in range(232):\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq] + states)\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_token = vi_loaded_tokenizer.index_word.get(sampled_token_index, '<unk>')\n",
    "            if sampled_token == '<eos>':\n",
    "                break\n",
    "    \n",
    "            decoded_sentence.append(sampled_token)\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "            states = [h, c]\n",
    "    \n",
    "        return ' '.join(decoded_sentence)\n",
    "    with open(directory_path + '/english_tokenizer.pkl', 'rb') as file:\n",
    "        eng_loaded_tokenizer = pickle.load(file)\n",
    "    with open(directory_path +'/vietnamese_tokenizer.pkl', 'rb') as file:\n",
    "        vi_loaded_tokenizer = pickle.load(file)\n",
    "    \n",
    "\n",
    "    for item in test_data:\n",
    "        source = item[\"en\"]\n",
    "        target = item[\"vi\"]\n",
    "        \n",
    "        input_sentence = source\n",
    "        input_sequence = preprocess_sentence(input_sentence, eng_loaded_tokenizer, 193)\n",
    "        \n",
    "        prediction = decode_sequence(input_sequence)\n",
    "            \n",
    "        texts.append(source)\n",
    "        predictions.append(prediction)\n",
    "        references.append(target)\n",
    "        \n",
    "    \n",
    "    return texts, predictions, references   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-GRU with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer, AutoModel, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SOS_token = 0\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size).to(device)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True).to(device)\n",
    "        self.hidden_transform = nn.Linear(hidden_size * 2, hidden_size).to(device)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.to(device)\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        output, hidden = output.to(device), hidden.to(device)\n",
    "        output = self.hidden_transform(output)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size).to(device)  # Linear layer for the query\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size).to(device)    # Linear layer for the keys\n",
    "        self.Va = nn.Linear(hidden_size, hidden_size).to(device)  # Linear layer for the values\n",
    "        self.softmax = nn.Softmax(dim=-1).to(device)  \n",
    "\n",
    "    def forward(self, query, keys):\n",
    "\n",
    "        query_proj = self.Wa(query).to(device)  # Shape: (batch_size, query_len, hidden_size)\n",
    "        key_proj = self.Ua(keys).to(device)      # Shape: (batch_size, key_len, hidden_size)\n",
    "        value_proj = self.Va(keys).to(device)  # Shape: (batch_size, key_len, hidden_size)\n",
    "\n",
    "        scores = torch.bmm(query_proj, key_proj.transpose(1, 2)).to(device)  # Shape: (batch_size, query_len, key_len)\n",
    "        scores = scores / torch.sqrt(torch.tensor(key_proj.size(-1), dtype=torch.float32, device=device))  # Scale by sqrt(hidden_size)\n",
    "        \n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = self.softmax(scores).to(device)  # Shape: (batch_size, query_len, key_len)\n",
    "\n",
    "        # Compute context vectors as weighted sum of values\n",
    "        context = torch.bmm(attention_weights, value_proj).to(device)  # Shape: (batch_size, query_len, hidden_size)\n",
    "\n",
    "        return context, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size).to(device)\n",
    "        self.attention = CrossAttention(hidden_size).to(device)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout_p), batch_first=True).to(device)\n",
    "        self.out = nn.Linear(hidden_size, output_size).to(device)\n",
    "        self.dropout = nn.Dropout(dropout_p).to(device)\n",
    "        self.hidden_transform = nn.Linear(hidden_size * 2, hidden_size).to(device)\n",
    "        self.hidden_input_transform = nn.Linear(hidden_size * 2, hidden_size).to(device)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        encoder_outputs = encoder_outputs.to(device)\n",
    "        encoder_hidden = encoder_hidden.to(device)\n",
    "        if target_tensor is not None:\n",
    "            target_tensor = target_tensor.to(device)\n",
    "\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token).to(device)\n",
    "        decoder_hidden = self.transform_bidirectional_hidden(encoder_hidden)\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1).to(device)  # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach().to(device)  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1).to(device)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1).to(device)\n",
    "        attentions = torch.cat(attentions, dim=1).to(device)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def transform_bidirectional_hidden(self, encoder_hidden):\n",
    "        forward_states = encoder_hidden[0::2, :, :].to(device)  # Forward states: (batch, num_layers, hidden_size)\n",
    "        backward_states = encoder_hidden[1::2, :, :].to(device)  # Backward states: (batch, num_layers, hidden_size)\n",
    "        combined_hidden = torch.cat((forward_states, backward_states), dim=2).to(device)  # Shape: (batch, num_layers, hidden_size * 2)\n",
    "        combined_hidden = self.hidden_transform(combined_hidden).to(device)\n",
    "        return combined_hidden\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        encoder_outputs = encoder_outputs.to(device)\n",
    "\n",
    "        embedded = self.embedding(input).to(device)\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "        input_gru = self.hidden_input_transform(input_gru)\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Translator(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Translator, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor=None):\n",
    "        if target_tensor is not None:\n",
    "            target_tensor = target_tensor.to(self.device)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = self.decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "        return decoder_outputs\n",
    "\n",
    "    def eval(self):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model_GRU(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_decoded_sentence(sentence):\n",
    "    special_tokens = [\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"]\n",
    "    for token in special_tokens:\n",
    "        sentence = sentence.replace(token, \"\").strip() \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate_GRU(input_text):\n",
    "    encoder_pth = \"/kaggle/working/model/models--Sag1012--machine-translation/snapshots/c5f85377fb64307c86c935bae0edf64b764d8db8/GRU_with_attention ver3/encoder.pth\"\n",
    "    decoder_pth = \"/kaggle/working/model/models--Sag1012--machine-translation/snapshots/c5f85377fb64307c86c935bae0edf64b764d8db8/GRU_with_attention ver3/decoder.pth\"\n",
    "    VOCAB_SIZE = 64000\n",
    "    hidden_size = 256\n",
    "    encoder = EncoderRNN(VOCAB_SIZE, hidden_size)\n",
    "    decoder = AttnDecoderRNN(hidden_size, VOCAB_SIZE)\n",
    "    load_model_GRU(encoder, encoder_pth)\n",
    "    load_model_GRU(decoder, decoder_pth)\n",
    "    translator = Translator(encoder,decoder,device)\n",
    "    english_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    vietnamese_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "    english_tokens = english_tokenizer.encode(input_text)\n",
    "    english_tensor = torch.tensor(english_tokens).unsqueeze(0).to(device)\n",
    "    print(device)\n",
    "    with torch.no_grad():\n",
    "        output_tensor = translator(english_tensor)\n",
    "    predicted_token_ids = torch.argmax(output_tensor, dim=-1).squeeze(0).tolist()\n",
    "    vietnamese_sentence = vietnamese_tokenizer.decode(predicted_token_ids)\n",
    "    vietnamese_sentence_cleaned = clean_decoded_sentence(vietnamese_sentence)\n",
    "    \n",
    "    return vietnamese_sentence_cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_predict_GRU(model_path,test_data):\n",
    "    import os\n",
    "    directory_path = os.path.dirname(model_path)\n",
    "    encoder_pth = directory_path + \"/encoder.pth\"\n",
    "    decoder_pth = directory_path + \"/decoder.pth\"\n",
    "    \n",
    "    texts = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    VOCAB_SIZE = 64000\n",
    "    hidden_size = 256\n",
    "    encoder = EncoderRNN(VOCAB_SIZE, hidden_size)\n",
    "    decoder = AttnDecoderRNN(hidden_size, VOCAB_SIZE)\n",
    "    load_model_GRU(encoder, encoder_pth)\n",
    "    load_model_GRU(decoder, decoder_pth)\n",
    "    translator = Translator(encoder,decoder,device)\n",
    "    english_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    vietnamese_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "    for item in test_data:\n",
    "        source = item[\"en\"]\n",
    "        target = item[\"vi\"]\n",
    "        \n",
    "        input_sentence = source\n",
    "        english_tokens = english_tokenizer.encode(input_sentence)\n",
    "        english_tensor = torch.tensor(english_tokens).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output_tensor = translator(english_tensor)\n",
    "        predicted_token_ids = torch.argmax(output_tensor, dim=-1).squeeze(0).tolist()\n",
    "        vietnamese_sentence = vietnamese_tokenizer.decode(predicted_token_ids)\n",
    "        vietnamese_sentence_cleaned = clean_decoded_sentence(vietnamese_sentence)\n",
    "                    \n",
    "        texts.append(source)\n",
    "        predictions.append(vietnamese_sentence_cleaned)\n",
    "        references.append(target)\n",
    "    return texts, predictions, references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marian MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:29:12.756340Z",
     "iopub.status.busy": "2024-12-15T04:29:12.755975Z",
     "iopub.status.idle": "2024-12-15T04:29:12.761101Z",
     "shell.execute_reply": "2024-12-15T04:29:12.760227Z",
     "shell.execute_reply.started": "2024-12-15T04:29:12.756309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def load_model_MarianMT(model_path):\n",
    "\n",
    "    tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-vi')\n",
    "    model = MarianMTModel.from_pretrained(model_path)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:29:16.194369Z",
     "iopub.status.busy": "2024-12-15T04:29:16.193999Z",
     "iopub.status.idle": "2024-12-15T04:29:16.201187Z",
     "shell.execute_reply": "2024-12-15T04:29:16.200269Z",
     "shell.execute_reply.started": "2024-12-15T04:29:16.194338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_predict_MarianMT(model_path, test_data):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer, model = load_model_MarianMT(model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    texts = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for item in test_data:\n",
    "        source = item[\"en\"] \n",
    "        target = item[\"vi\"] \n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(source, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        # Generate translation\n",
    "        outputs = model.generate(**inputs, max_length=64, num_beams=4)\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        texts.append(source)\n",
    "        predictions.append(prediction)\n",
    "        references.append(target)\n",
    "    \n",
    "    return texts, predictions, references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_vi = AutoTokenizer.from_pretrained(\"vinai/phobert-base\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(input_sequence, model, tokenizer_target, max_length=50):\n",
    "\n",
    "    input_sequence = tf.constant([input_sequence], dtype=tf.int64)\n",
    "\n",
    "    start_token = tokenizer_vi.cls_token_id\n",
    "    end_token = tokenizer_vi.sep_token_id\n",
    "\n",
    "    target_sequence = [start_token]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        decoder_input = tf.constant([target_sequence], dtype=tf.int64)\n",
    "        predictions = model.predict([input_sequence, decoder_input], verbose=0)\n",
    "        next_token = tf.argmax(predictions[:, -1, :], axis=-1).numpy()[0]\n",
    "        target_sequence.append(next_token)\n",
    "\n",
    "        if next_token == end_token:\n",
    "            break\n",
    "    translated_sentence = tokenizer_target.decode(target_sequence[1:], skip_special_tokens=True)\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_lstm_attention(model_path, test_data):\n",
    "    model = load_model(model_path)\n",
    "    tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_vi = AutoTokenizer.from_pretrained(\"vinai/phobert-base\") \n",
    "\n",
    "    texts = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for item in test_data:\n",
    "        source = item[\"en\"] \n",
    "        target = item[\"vi\"]\n",
    "        input_sequence = tokenizer_en.encode(source, add_special_tokens=True )\n",
    "  \n",
    "        prediction = greedy_decode(input_sequence, model, tokenizer_vi)\n",
    "        \n",
    "        texts.append(source)\n",
    "        predictions.append(prediction)\n",
    "        references.append(target)\n",
    "    \n",
    "    return texts, predictions, references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosin similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:29:18.352604Z",
     "iopub.status.busy": "2024-12-15T04:29:18.352265Z",
     "iopub.status.idle": "2024-12-15T04:29:18.359555Z",
     "shell.execute_reply": "2024-12-15T04:29:18.358559Z",
     "shell.execute_reply.started": "2024-12-15T04:29:18.352574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(predictions, references):\n",
    "    from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "    from transformers import  AutoTokenizer, AutoModel\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    decoder_model_name = \"vinai/bartpho-word\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\n",
    "    model = AutoModel.from_pretrained(decoder_model_name)\n",
    "\n",
    "    # Move the model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    cos = []\n",
    "\n",
    "    # Process each prediction and reference pair\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Tokenize predictions and references\n",
    "        p = tokenizer(pred, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        p.pop(\"token_type_ids\", None)\n",
    "        r = tokenizer(ref, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        r.pop(\"token_type_ids\", None)\n",
    "\n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings1 = model(**p).last_hidden_state.mean(dim=1)  # Prediction embeddings\n",
    "            embeddings2 = model(**r).last_hidden_state.mean(dim=1)  # Reference embeddings\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = torch_cosine_similarity(embeddings1, embeddings2).item()  # Use PyTorch cosine similarity\n",
    "        cos.append(similarity)\n",
    "\n",
    "    return cos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:29:20.464546Z",
     "iopub.status.busy": "2024-12-15T04:29:20.463836Z",
     "iopub.status.idle": "2024-12-15T04:29:20.469412Z",
     "shell.execute_reply": "2024-12-15T04:29:20.468559Z",
     "shell.execute_reply.started": "2024-12-15T04:29:20.464510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu_scores(predictions, references, weights = (1,0,0,0)):\n",
    "\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    BLEU_scores = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        ref_tokens = ref.split()\n",
    "        pred_tokens = pred.split()\n",
    "\n",
    "        bleu = sentence_bleu([ref_tokens], pred_tokens, weights=weights, smoothing_function=smoothing)\n",
    "        BLEU_scores.append(bleu)\n",
    "    \n",
    "    return BLEU_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:29:23.504662Z",
     "iopub.status.busy": "2024-12-15T04:29:23.503897Z",
     "iopub.status.idle": "2024-12-15T04:29:23.510571Z",
     "shell.execute_reply": "2024-12-15T04:29:23.509527Z",
     "shell.execute_reply.started": "2024-12-15T04:29:23.504627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, model_path, test_data):\n",
    "    fn = globals()[f\"get_predict_{model_name}\"]\n",
    "    texts, predictions, references = fn(model_path, test_data)\n",
    "    cos = cos_sim(predictions, references)\n",
    "    scores1 = calculate_bleu_scores(predictions, references, weights = (1,0,0,0))\n",
    "    scores2 = calculate_bleu_scores(predictions, references, weights = (0.5,0.5,0,0))\n",
    "    scores3 = calculate_bleu_scores(predictions, references, weights = (0.5,0.25,0.25,0))\n",
    "    scores4 = calculate_bleu_scores(predictions, references, weights = (0.25,0.25,0.25,0.25))\n",
    "    \n",
    "    data ={\n",
    "         'texts': texts,\n",
    "         'predictions': predictions,\n",
    "         'references': references,\n",
    "         'BLEU_1': scores1,\n",
    "         'BLEU_2': scores2,\n",
    "        'BLEU_3': scores3,\n",
    "        'BLEU_4': scores4,\n",
    "         \"cosin similarity\": cos\n",
    "     }\n",
    "        \n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:29:25.637300Z",
     "iopub.status.busy": "2024-12-15T04:29:25.636852Z",
     "iopub.status.idle": "2024-12-15T04:29:54.983891Z",
     "shell.execute_reply": "2024-12-15T04:29:54.982963Z",
     "shell.execute_reply.started": "2024-12-15T04:29:25.637267Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861b570563474bcba77d627898a00b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24d139dbe7646b0860d4215061b0d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/809k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab8ed6a46154c0cac0e540c9b2b4fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/756k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9689d6afe0c4a3984a59c1ba15e1c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc39cccd72f4b8abe8e7bb327d57fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'MarianMT'  # Model name of model\n",
    "model_path= '/kaggle/working/model/models--Sag1012--machine-translation/snapshots/dff7854613e72ee87d975bc13c01813f05dd3dc5/MarianMT_ver2'   # Model Path of model\n",
    "\n",
    "df = evaluate_model(model_name, model_path, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:30:16.465238Z",
     "iopub.status.busy": "2024-12-15T04:30:16.464523Z",
     "iopub.status.idle": "2024-12-15T04:30:16.488894Z",
     "shell.execute_reply": "2024-12-15T04:30:16.488121Z",
     "shell.execute_reply.started": "2024-12-15T04:30:16.465203Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>predictions</th>\n",
       "      <th>references</th>\n",
       "      <th>BLEU_1</th>\n",
       "      <th>BLEU_2</th>\n",
       "      <th>BLEU_3</th>\n",
       "      <th>BLEU_4</th>\n",
       "      <th>cosin similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Murillo has served as the Nicaraguan governmen...</td>\n",
       "      <td>Murillo đã từng là người phát ngôn chính của c...</td>\n",
       "      <td>Murillo đã từng là phát ngôn viên chính của ch...</td>\n",
       "      <td>0.817283</td>\n",
       "      <td>0.716367</td>\n",
       "      <td>0.668703</td>\n",
       "      <td>0.556204</td>\n",
       "      <td>0.969939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Work : People may regret not following a diffe...</td>\n",
       "      <td>Công việc : Người ta có thể hối tiếc vì không ...</td>\n",
       "      <td>Công việc : Con người có thể cảm thấy hối tiếc...</td>\n",
       "      <td>0.641245</td>\n",
       "      <td>0.494335</td>\n",
       "      <td>0.436449</td>\n",
       "      <td>0.297307</td>\n",
       "      <td>0.921119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M- my wife barely lets me see her naked .</td>\n",
       "      <td>Vợ tôi hầu như không cho tôi thấy cô ấy trần t...</td>\n",
       "      <td>Vợ ... vợ tôi còn hiếm khi để tôi nhìn cô ấy k...</td>\n",
       "      <td>0.395725</td>\n",
       "      <td>0.168151</td>\n",
       "      <td>0.096638</td>\n",
       "      <td>0.037076</td>\n",
       "      <td>0.717559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It 's full of panic and fear , and I 'd heard ...</td>\n",
       "      <td>Nó đầy hoảng loạn và sợ hãi , và tôi nghe thấy...</td>\n",
       "      <td>Nó chứa đầy sợ hãi và hoảng loạn và tôi nghe t...</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.504975</td>\n",
       "      <td>0.387803</td>\n",
       "      <td>0.110886</td>\n",
       "      <td>0.898981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This will make it much easier for you to expos...</td>\n",
       "      <td>Điều này sẽ giúp bạn dễ dàng vạch trần quả bón...</td>\n",
       "      <td>Bước này sẽ giúp bạn dễ dàng làm lộ rễ cây để ...</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.412861</td>\n",
       "      <td>0.366260</td>\n",
       "      <td>0.257534</td>\n",
       "      <td>0.841695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>You can click on any text box and start typing...</td>\n",
       "      <td>Bạn có thể nhấp vào hộp văn bản và bắt đầu gõ ...</td>\n",
       "      <td>Bạn có thể nhấp chuột vào bất kỳ hộp thoại nào...</td>\n",
       "      <td>0.619198</td>\n",
       "      <td>0.433439</td>\n",
       "      <td>0.403613</td>\n",
       "      <td>0.280517</td>\n",
       "      <td>0.850169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Yeah , they 're rocket scientists , remember ?</td>\n",
       "      <td>Phải , họ là những nhà khoa học tên lửa , nhớ ...</td>\n",
       "      <td>Ừ , họ là các nhà khoa học tên lửa , nhớ không ?</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.811998</td>\n",
       "      <td>0.783462</td>\n",
       "      <td>0.699752</td>\n",
       "      <td>0.915851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Fidgeting is a sign that you lack confidence .</td>\n",
       "      <td>Tán tỉnh là dấu hiệu cho thấy bạn thiếu tự tin .</td>\n",
       "      <td>Đó là dấu hiệu cho thấy sự thiếu tự tin của bạn .</td>\n",
       "      <td>0.766704</td>\n",
       "      <td>0.620294</td>\n",
       "      <td>0.574014</td>\n",
       "      <td>0.412491</td>\n",
       "      <td>0.807538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Agreeing with the angry person might help diff...</td>\n",
       "      <td>Đồng ý với người có tính nóng giận có thể giúp...</td>\n",
       "      <td>Việc bày tỏ sự đồng tình với người đang tức gi...</td>\n",
       "      <td>0.378733</td>\n",
       "      <td>0.259808</td>\n",
       "      <td>0.221808</td>\n",
       "      <td>0.134033</td>\n",
       "      <td>0.766936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Russell , is going to lose his lectureship as ...</td>\n",
       "      <td>Russell , sẽ mất chức giảng viên do kết quả củ...</td>\n",
       "      <td>Russell , sẽ mất quyền giảng dạy vì cái vụ phá...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.324443</td>\n",
       "      <td>0.276536</td>\n",
       "      <td>0.161956</td>\n",
       "      <td>0.777876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                texts  \\\n",
       "0   Murillo has served as the Nicaraguan governmen...   \n",
       "1   Work : People may regret not following a diffe...   \n",
       "2           M- my wife barely lets me see her naked .   \n",
       "3   It 's full of panic and fear , and I 'd heard ...   \n",
       "4   This will make it much easier for you to expos...   \n",
       "..                                                ...   \n",
       "95  You can click on any text box and start typing...   \n",
       "96     Yeah , they 're rocket scientists , remember ?   \n",
       "97     Fidgeting is a sign that you lack confidence .   \n",
       "98  Agreeing with the angry person might help diff...   \n",
       "99  Russell , is going to lose his lectureship as ...   \n",
       "\n",
       "                                          predictions  \\\n",
       "0   Murillo đã từng là người phát ngôn chính của c...   \n",
       "1   Công việc : Người ta có thể hối tiếc vì không ...   \n",
       "2   Vợ tôi hầu như không cho tôi thấy cô ấy trần t...   \n",
       "3   Nó đầy hoảng loạn và sợ hãi , và tôi nghe thấy...   \n",
       "4   Điều này sẽ giúp bạn dễ dàng vạch trần quả bón...   \n",
       "..                                                ...   \n",
       "95  Bạn có thể nhấp vào hộp văn bản và bắt đầu gõ ...   \n",
       "96  Phải , họ là những nhà khoa học tên lửa , nhớ ...   \n",
       "97   Tán tỉnh là dấu hiệu cho thấy bạn thiếu tự tin .   \n",
       "98  Đồng ý với người có tính nóng giận có thể giúp...   \n",
       "99  Russell , sẽ mất chức giảng viên do kết quả củ...   \n",
       "\n",
       "                                           references    BLEU_1    BLEU_2  \\\n",
       "0   Murillo đã từng là phát ngôn viên chính của ch...  0.817283  0.716367   \n",
       "1   Công việc : Con người có thể cảm thấy hối tiếc...  0.641245  0.494335   \n",
       "2   Vợ ... vợ tôi còn hiếm khi để tôi nhìn cô ấy k...  0.395725  0.168151   \n",
       "3   Nó chứa đầy sợ hãi và hoảng loạn và tôi nghe t...  0.680000  0.504975   \n",
       "4   Bước này sẽ giúp bạn dễ dàng làm lộ rễ cây để ...  0.545455  0.412861   \n",
       "..                                                ...       ...       ...   \n",
       "95  Bạn có thể nhấp chuột vào bất kỳ hộp thoại nào...  0.619198  0.433439   \n",
       "96   Ừ , họ là các nhà khoa học tên lửa , nhớ không ?  0.857143  0.811998   \n",
       "97  Đó là dấu hiệu cho thấy sự thiếu tự tin của bạn .  0.766704  0.620294   \n",
       "98  Việc bày tỏ sự đồng tình với người đang tức gi...  0.378733  0.259808   \n",
       "99  Russell , sẽ mất quyền giảng dạy vì cái vụ phá...  0.500000  0.324443   \n",
       "\n",
       "      BLEU_3    BLEU_4  cosin similarity  \n",
       "0   0.668703  0.556204          0.969939  \n",
       "1   0.436449  0.297307          0.921119  \n",
       "2   0.096638  0.037076          0.717559  \n",
       "3   0.387803  0.110886          0.898981  \n",
       "4   0.366260  0.257534          0.841695  \n",
       "..       ...       ...               ...  \n",
       "95  0.403613  0.280517          0.850169  \n",
       "96  0.783462  0.699752          0.915851  \n",
       "97  0.574014  0.412491          0.807538  \n",
       "98  0.221808  0.134033          0.766936  \n",
       "99  0.276536  0.161956          0.777876  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:30:25.244639Z",
     "iopub.status.busy": "2024-12-15T04:30:25.244322Z",
     "iopub.status.idle": "2024-12-15T04:30:25.251588Z",
     "shell.execute_reply": "2024-12-15T04:30:25.250611Z",
     "shell.execute_reply.started": "2024-12-15T04:30:25.244613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6070899981850879"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print average scores of BLEU 1\n",
    "\n",
    "df['BLEU_1'].sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:30:26.850255Z",
     "iopub.status.busy": "2024-12-15T04:30:26.849896Z",
     "iopub.status.idle": "2024-12-15T04:30:26.856841Z",
     "shell.execute_reply": "2024-12-15T04:30:26.855823Z",
     "shell.execute_reply.started": "2024-12-15T04:30:26.850222Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4849884020116053"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print average scores of BLEU 2\n",
    "\n",
    "df['BLEU_2'].sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:30:28.515924Z",
     "iopub.status.busy": "2024-12-15T04:30:28.515574Z",
     "iopub.status.idle": "2024-12-15T04:30:28.522414Z",
     "shell.execute_reply": "2024-12-15T04:30:28.521474Z",
     "shell.execute_reply.started": "2024-12-15T04:30:28.515891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43604474040861013"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print average scores of BLEU 3\n",
    "\n",
    "df['BLEU_3'].sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:30:30.175185Z",
     "iopub.status.busy": "2024-12-15T04:30:30.174274Z",
     "iopub.status.idle": "2024-12-15T04:30:30.183565Z",
     "shell.execute_reply": "2024-12-15T04:30:30.182121Z",
     "shell.execute_reply.started": "2024-12-15T04:30:30.175127Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33090962475722635"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print average scores of BLEU 4\n",
    "\n",
    "df['BLEU_4'].sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T04:30:32.674096Z",
     "iopub.status.busy": "2024-12-15T04:30:32.673743Z",
     "iopub.status.idle": "2024-12-15T04:30:32.680527Z",
     "shell.execute_reply": "2024-12-15T04:30:32.679550Z",
     "shell.execute_reply.started": "2024-12-15T04:30:32.674052Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441271716356278"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print average scores of cosin similarity\n",
    "\n",
    "df['cosin similarity'].sum()/len(df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6303181,
     "sourceId": 10200339,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 191281,
     "modelInstanceId": 168929,
     "sourceId": 198072,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
