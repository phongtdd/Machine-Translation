{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:44.957423Z",
     "iopub.status.busy": "2024-12-15T12:09:44.957045Z",
     "iopub.status.idle": "2024-12-15T12:09:44.963126Z",
     "shell.execute_reply": "2024-12-15T12:09:44.962008Z",
     "shell.execute_reply.started": "2024-12-15T12:09:44.957384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:45.274028Z",
     "iopub.status.busy": "2024-12-15T12:09:45.273231Z",
     "iopub.status.idle": "2024-12-15T12:09:45.281903Z",
     "shell.execute_reply": "2024-12-15T12:09:45.280984Z",
     "shell.execute_reply.started": "2024-12-15T12:09:45.273995Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer, AutoModel, PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:45.494926Z",
     "iopub.status.busy": "2024-12-15T12:09:45.494620Z",
     "iopub.status.idle": "2024-12-15T12:09:45.501011Z",
     "shell.execute_reply": "2024-12-15T12:09:45.500114Z",
     "shell.execute_reply.started": "2024-12-15T12:09:45.494900Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:45.684667Z",
     "iopub.status.busy": "2024-12-15T12:09:45.684378Z",
     "iopub.status.idle": "2024-12-15T12:09:45.690015Z",
     "shell.execute_reply": "2024-12-15T12:09:45.689054Z",
     "shell.execute_reply.started": "2024-12-15T12:09:45.684636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:45.854825Z",
     "iopub.status.busy": "2024-12-15T12:09:45.854083Z",
     "iopub.status.idle": "2024-12-15T12:09:45.859887Z",
     "shell.execute_reply": "2024-12-15T12:09:45.859130Z",
     "shell.execute_reply.started": "2024-12-15T12:09:45.854796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:46.049052Z",
     "iopub.status.busy": "2024-12-15T12:09:46.048431Z",
     "iopub.status.idle": "2024-12-15T12:09:46.535051Z",
     "shell.execute_reply": "2024-12-15T12:09:46.534071Z",
     "shell.execute_reply.started": "2024-12-15T12:09:46.049020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vi_sentences_path = \"/kaggle/input/berttokenize/Bert/tokenize_vi.txt\"\n",
    "en_sentences_path = \"/kaggle/input/berttokenize/Bert/tokenize_en.txt\"\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_vi = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:46.536905Z",
     "iopub.status.busy": "2024-12-15T12:09:46.536635Z",
     "iopub.status.idle": "2024-12-15T12:09:46.541594Z",
     "shell.execute_reply": "2024-12-15T12:09:46.540813Z",
     "shell.execute_reply.started": "2024-12-15T12:09:46.536879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnamese Vocabulary Size: 64000\n",
      "English Vocabulary Size: 30522\n"
     ]
    }
   ],
   "source": [
    "vi_vocab_size = tokenizer_vi.vocab_size\n",
    "en_vocab_size = tokenizer_en.vocab_size\n",
    "\n",
    "print(f\"Vietnamese Vocabulary Size: {vi_vocab_size}\")\n",
    "print(f\"English Vocabulary Size: {en_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:46.542868Z",
     "iopub.status.busy": "2024-12-15T12:09:46.542565Z",
     "iopub.status.idle": "2024-12-15T12:09:46.552974Z",
     "shell.execute_reply": "2024-12-15T12:09:46.552291Z",
     "shell.execute_reply.started": "2024-12-15T12:09:46.542844Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2448,   999,  2522, 14343,   999,   102]])\n",
      "Decoded Text: run! corre!\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Run!\tCorre!\"\n",
    "\n",
    "input_ids = tokenizer_en.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "decoded_text = tokenizer_en.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:46.592267Z",
     "iopub.status.busy": "2024-12-15T12:09:46.592038Z",
     "iopub.status.idle": "2024-12-15T12:09:46.600406Z",
     "shell.execute_reply": "2024-12-15T12:09:46.599646Z",
     "shell.execute_reply.started": "2024-12-15T12:09:46.592245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,   321,   387,  4698,  1626,   157,    31, 28171,  1187,     4,\n",
      "            49,  7303,  5761,   139,    64, 52479, 10507, 15275,  1624,  1340,\n",
      "         11095,  1517,   528,   355,   157,     6,  1278,  1624,  1340, 11095,\n",
      "          1517,    44,   528,   355,   350,     8,    59,  2665, 37096, 10838,\n",
      "          2543,    97,     8,   455,   376,  1591,  4623,     2]])\n",
      "Decoded Text: Với bài toán dịch Anh - Việt, việc kiểm tra cách mà tokenizer mã hóa câu tiếng Anh và tái mã hóa lại câu tiếng Việt là rất quan trọng. Dưới đây là hướng dẫn cụ thể\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Với bài toán dịch Anh - Việt, việc kiểm tra cách mà tokenizer mã hóa câu tiếng Anh và tái mã hóa lại câu tiếng Việt là rất quan trọng. Dưới đây là hướng dẫn cụ thể\"\n",
    "\n",
    "input_ids = tokenizer_vi.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "decoded_text = tokenizer_vi.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:46.793392Z",
     "iopub.status.busy": "2024-12-15T12:09:46.793041Z",
     "iopub.status.idle": "2024-12-15T12:09:48.793987Z",
     "shell.execute_reply": "2024-12-15T12:09:48.792998Z",
     "shell.execute_reply.started": "2024-12-15T12:09:46.793364Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in tokenized vietnamese: 2977999\n",
      "Number of sentences in tokenized english: 2977999\n"
     ]
    }
   ],
   "source": [
    "def count_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return len(lines)\n",
    "\n",
    "\n",
    "num_sentences_vi = count_sentences(vi_sentences_path)\n",
    "num_sentences_en = count_sentences(en_sentences_path)\n",
    "\n",
    "print(f\"Number of sentences in tokenized vietnamese: {num_sentences_vi}\")\n",
    "print(f\"Number of sentences in tokenized english: {num_sentences_en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:48.796257Z",
     "iopub.status.busy": "2024-12-15T12:09:48.795860Z",
     "iopub.status.idle": "2024-12-15T12:09:48.828726Z",
     "shell.execute_reply": "2024-12-15T12:09:48.827792Z",
     "shell.execute_reply.started": "2024-12-15T12:09:48.796219Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 tokens in the English vocabulary:\n",
      "1. ##rce\n",
      "2. insults\n",
      "3. hugh\n",
      "4. √\n",
      "5. ##ened\n",
      "6. three\n",
      "7. ட\n",
      "8. rub\n",
      "9. ancestral\n",
      "10. ##qu\n",
      "11. langley\n",
      "12. selfish\n",
      "13. ##pton\n",
      "14. pcs\n",
      "15. ##alia\n",
      "16. differs\n",
      "17. morris\n",
      "18. ##wee\n",
      "19. cuts\n",
      "20. categorized\n",
      "\n",
      "First 20 tokens in the Vietnamese vocabulary:\n",
      "1. <s>\n",
      "2. <pad>\n",
      "3. </s>\n",
      "4. <unk>\n",
      "5. ,\n",
      "6. .\n",
      "7. và\n",
      "8. của\n",
      "9. là\n",
      "10. các\n",
      "11. có\n",
      "12. được\n",
      "13. trong\n",
      "14. cho\n",
      "15. đã\n",
      "16. với\n",
      "17. một\n",
      "18. không\n",
      "19. người\n",
      "20. )\n"
     ]
    }
   ],
   "source": [
    "vi_vocab = tokenizer_vi.get_vocab()  \n",
    "en_vocab = tokenizer_en.get_vocab()  \n",
    "\n",
    "print(\"First 20 tokens in the English vocabulary:\")\n",
    "for i, (token, _) in enumerate(list(en_vocab.items())[:20]):\n",
    "    print(f\"{i+1}. {token}\")\n",
    "\n",
    "print(\"\\nFirst 20 tokens in the Vietnamese vocabulary:\")\n",
    "for i, (token, _) in enumerate(list(vi_vocab.items())[:20]):\n",
    "    print(f\"{i+1}. {token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:09:48.830369Z",
     "iopub.status.busy": "2024-12-15T12:09:48.829890Z",
     "iopub.status.idle": "2024-12-15T12:10:46.999801Z",
     "shell.execute_reply": "2024-12-15T12:10:46.998973Z",
     "shell.execute_reply.started": "2024-12-15T12:09:48.830330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_tokenized_sentences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "    return [list(map(int, line.strip().split())) for line in lines]\n",
    "\n",
    "tokenized_en = read_tokenized_sentences(en_sentences_path)\n",
    "tokenized_vi = read_tokenized_sentences(vi_sentences_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.002149Z",
     "iopub.status.busy": "2024-12-15T12:10:47.001839Z",
     "iopub.status.idle": "2024-12-15T12:10:47.167729Z",
     "shell.execute_reply": "2024-12-15T12:10:47.166995Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.002122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sample_data(english_sentences, vietnamese_sentences, sample_ratio=0.015):\n",
    "    dataset_size = len(english_sentences)\n",
    "    sample_size = int(sample_ratio * dataset_size)\n",
    "    indices = np.random.choice(dataset_size, sample_size, replace=False)\n",
    "\n",
    "    sampled_en = [english_sentences[i] for i in indices]\n",
    "    sampled_vi = [vietnamese_sentences[i] for i in indices]\n",
    "\n",
    "    return sampled_en, sampled_vi\n",
    "\n",
    "sampled_en, sampled_vi = sample_data(tokenized_en, tokenized_vi, sample_ratio=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.168966Z",
     "iopub.status.busy": "2024-12-15T12:10:47.168697Z",
     "iopub.status.idle": "2024-12-15T12:10:47.499204Z",
     "shell.execute_reply": "2024-12-15T12:10:47.498273Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.168917Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English batch: tensor([[ 101, 2043, 2057,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 1996,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2572,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2292, 1055,  ...,    0,    0,    0],\n",
      "        [ 101, 1999, 2456,  ..., 2368, 1010,  102],\n",
      "        [ 101, 2505, 2842,  ...,    0,    0,    0]], device='cuda:0')\n",
      "Vietnamese batch: tensor([[    0,   251,   572,  ...,     1,     1,     1],\n",
      "        [    0,  2510,  7493,  ...,     1,     1,     1],\n",
      "        [    0,  8051,     8,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 12127,    70,  ...,     1,     1,     1],\n",
      "        [    0,   125,   266,  ...,  4704,  1493,     2],\n",
      "        [    0,   631,   148,  ...,     1,     1,     1]], device='cuda:0')\n",
      "tensor([[  101,  2129, 15140,  ...,     0,     0,     0],\n",
      "        [  101,  2017, 22374,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2064,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1996,  3537,  ...,     0,     0,     0],\n",
      "        [  101,  2043,  2017,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1040,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([[    0,   453,  1800,  ...,     1,     1,     1],\n",
      "        [    0,  1350,    14,  ...,     1,     1,     1],\n",
      "        [    0,   218,    10,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 21728, 14318,  ...,    57,    30,     2],\n",
      "        [    0,   251,    10,  ...,     1,     1,     1],\n",
      "        [    0,   218,    17,  ...,     1,     1,     1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, english_sentences, vietnamese_sentences, device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            english_sentences (list): List of English sentence tokenized lists.\n",
    "            vietnamese_sentences (list): List of Vietnamese sentence tokenized lists.\n",
    "            device (str): The device to which tensors should be moved ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.english_sentences = english_sentences\n",
    "        self.vietnamese_sentences = vietnamese_sentences\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en_sentence = torch.tensor(self.english_sentences[idx], dtype=torch.long, device=self.device)\n",
    "        vi_sentence = torch.tensor(self.vietnamese_sentences[idx], dtype=torch.long, device=self.device)\n",
    "        \n",
    "        return en_sentence, vi_sentence\n",
    "\n",
    "def create_pytorch_dataset(english_sentences, vietnamese_sentences, train_split=0.8, device='cpu'):\n",
    "    dataset_size = len(english_sentences)\n",
    "    indices = np.arange(dataset_size)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_size = int(train_split * dataset_size)\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "\n",
    "    train_dataset = TranslationDataset(\n",
    "        [english_sentences[i] for i in train_indices],\n",
    "        [vietnamese_sentences[i] for i in train_indices],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    val_dataset = TranslationDataset(\n",
    "        [english_sentences[i] for i in val_indices],\n",
    "        [vietnamese_sentences[i] for i in val_indices],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "train_dataset, val_dataset = create_pytorch_dataset(sampled_en, sampled_vi, train_split=0.8, device = device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for en_batch, vi_batch in train_dataloader:\n",
    "    print(\"English batch:\", en_batch)\n",
    "    print(\"Vietnamese batch:\", vi_batch)\n",
    "    break\n",
    "for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "    print(inputs)\n",
    "    print(targets)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.501123Z",
     "iopub.status.busy": "2024-12-15T12:10:47.500784Z",
     "iopub.status.idle": "2024-12-15T12:10:47.506138Z",
     "shell.execute_reply": "2024-12-15T12:10:47.505211Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.501078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    loss = F.cross_entropy(y_pred, y_true, reduction='none')\n",
    "\n",
    "    mask = (y_true != 0).float()\n",
    "\n",
    "    loss = loss * mask\n",
    "\n",
    "    return loss.sum() / mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.507606Z",
     "iopub.status.busy": "2024-12-15T12:10:47.507254Z",
     "iopub.status.idle": "2024-12-15T12:10:47.517027Z",
     "shell.execute_reply": "2024-12-15T12:10:47.516092Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.507578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def masked_acc(y_true, y_pred):\n",
    "    y_pred = torch.argmax(y_pred, dim=-1)\n",
    "    \n",
    "    mask = (y_true != 0).float()\n",
    "\n",
    "    correct = (y_true == y_pred).float()\n",
    "\n",
    "    correct = correct * mask\n",
    "\n",
    "    return correct.sum() / mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.518541Z",
     "iopub.status.busy": "2024-12-15T12:10:47.518207Z",
     "iopub.status.idle": "2024-12-15T12:10:47.527772Z",
     "shell.execute_reply": "2024-12-15T12:10:47.526991Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.518505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def bleu_score(y_true, y_pred):\n",
    "\n",
    "    y_true = y_true.cpu().numpy() if isinstance(y_true, torch.Tensor) else y_true\n",
    "    y_pred = y_pred.cpu().numpy() if isinstance(y_pred, torch.Tensor) else y_pred\n",
    "    \n",
    "    y_pred = torch.argmax(y_pred, dim=-1).cpu().numpy()\n",
    "\n",
    "    smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "    bleu_scores = []\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        reference = [y_true[i]] \n",
    "        candidate = y_pred[i]   \n",
    "\n",
    "        score = sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
    "        bleu_scores.append(score)\n",
    "\n",
    "    return torch.tensor(bleu_scores, dtype=torch.float32).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.529056Z",
     "iopub.status.busy": "2024-12-15T12:10:47.528789Z",
     "iopub.status.idle": "2024-12-15T12:10:47.541977Z",
     "shell.execute_reply": "2024-12-15T12:10:47.541095Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.529032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 64000\n",
    "UNITS = 512\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.544598Z",
     "iopub.status.busy": "2024-12-15T12:10:47.544324Z",
     "iopub.status.idle": "2024-12-15T12:10:47.552137Z",
     "shell.execute_reply": "2024-12-15T12:10:47.551312Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.544574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size).to(device)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True).to(device)\n",
    "        self.dropout = nn.Dropout(dropout_p).to(device)\n",
    "        self.hidden_transform = nn.Linear(hidden_size * 2, hidden_size).to(device)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.to(device)\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.gru(embedded)       \n",
    "        output = self.hidden_transform(output)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.553448Z",
     "iopub.status.busy": "2024-12-15T12:10:47.553168Z",
     "iopub.status.idle": "2024-12-15T12:10:47.563827Z",
     "shell.execute_reply": "2024-12-15T12:10:47.563009Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.553419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        assert hidden_size % num_heads == 0, \"Hidden size must be divisible by the number of heads.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.Wq = nn.Linear(hidden_size, hidden_size).to(device)  \n",
    "        self.Wk = nn.Linear(hidden_size, hidden_size).to(device)  \n",
    "        self.Wv = nn.Linear(hidden_size, hidden_size).to(device) \n",
    "\n",
    "        self.Wo = nn.Linear(hidden_size, hidden_size).to(device)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1).to(device)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        query = query.to(device)\n",
    "        keys = keys.to(device)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query_proj = self.Wq(query) \n",
    "        key_proj = self.Wk(keys)    \n",
    "        value_proj = self.Wv(keys)  \n",
    "\n",
    "        query_proj = query_proj.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_proj = key_proj.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value_proj = value_proj.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(query_proj, key_proj.transpose(-2, -1))  \n",
    "        scores = scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))  \n",
    "\n",
    "        attention_weights = self.softmax(scores)  \n",
    "\n",
    "        context = torch.matmul(attention_weights, value_proj) \n",
    "\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "        output = self.Wo(context)  \n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.565737Z",
     "iopub.status.busy": "2024-12-15T12:10:47.565376Z",
     "iopub.status.idle": "2024-12-15T12:10:47.607173Z",
     "shell.execute_reply": "2024-12-15T12:10:47.606128Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.565699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size).to(device)\n",
    "        self.attention = CrossAttention(hidden_size, num_heads=2).to(device)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,batch_first = True).to(device)\n",
    "        self.out = nn.Linear(hidden_size, output_size).to(device)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.hidden_transform = nn.Linear(hidden_size * 2, hidden_size).to(device)\n",
    "        self.hidden_input_transform = nn.Linear(hidden_size * 2, hidden_size).to(device)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        encoder_outputs = encoder_outputs.to(device)\n",
    "        encoder_hidden = encoder_hidden.to(device)\n",
    "        if target_tensor is not None:\n",
    "            target_tensor = target_tensor.to(device)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = self.transform_bidirectional_hidden(encoder_hidden)\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) \n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  \n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "    def transform_bidirectional_hidden(self, encoder_hidden):\n",
    "        forward_states = encoder_hidden[0::2, :, :]\n",
    "        backward_states = encoder_hidden[1::2, :, :] \n",
    "        combined_hidden = torch.cat((forward_states, backward_states), dim=2) \n",
    "        combined_hidden = self.hidden_transform(combined_hidden)\n",
    "        return combined_hidden\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "        input_gru = self.hidden_input_transform(input_gru)\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.608664Z",
     "iopub.status.busy": "2024-12-15T12:10:47.608328Z",
     "iopub.status.idle": "2024-12-15T12:10:47.620086Z",
     "shell.execute_reply": "2024-12-15T12:10:47.619223Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.608636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Translator(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Translator, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor=None):\n",
    "        if target_tensor is not None:\n",
    "            target_tensor = target_tensor.to(self.device)\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = self.decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "        return decoder_outputs\n",
    "\n",
    "    def eval(self):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.621627Z",
     "iopub.status.busy": "2024-12-15T12:10:47.621137Z",
     "iopub.status.idle": "2024-12-15T12:10:47.634711Z",
     "shell.execute_reply": "2024-12-15T12:10:47.633992Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.621588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:10:47.636064Z",
     "iopub.status.busy": "2024-12-15T12:10:47.635716Z",
     "iopub.status.idle": "2024-12-15T12:10:47.646133Z",
     "shell.execute_reply": "2024-12-15T12:10:47.645311Z",
     "shell.execute_reply.started": "2024-12-15T12:10:47.636027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:23:35.440645Z",
     "iopub.status.busy": "2024-12-15T12:23:35.439951Z",
     "iopub.status.idle": "2024-12-15T12:23:35.444693Z",
     "shell.execute_reply": "2024-12-15T12:23:35.443673Z",
     "shell.execute_reply.started": "2024-12-15T12:23:35.440611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:23:35.988726Z",
     "iopub.status.busy": "2024-12-15T12:23:35.988374Z",
     "iopub.status.idle": "2024-12-15T12:23:35.995023Z",
     "shell.execute_reply": "2024-12-15T12:23:35.994171Z",
     "shell.execute_reply.started": "2024-12-15T12:23:35.988695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "        predicted_token_ids = torch.argmax(decoder_outputs, dim=-1)[0].tolist()\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:23:36.031144Z",
     "iopub.status.busy": "2024-12-15T12:23:36.030243Z",
     "iopub.status.idle": "2024-12-15T12:23:36.036169Z",
     "shell.execute_reply": "2024-12-15T12:23:36.035315Z",
     "shell.execute_reply.started": "2024-12-15T12:23:36.031112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def val_epoch(dataloader, encoder, decoder, criterion):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            input_tensor, target_tensor = data\n",
    "    \n",
    "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "    \n",
    "            loss = criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_tensor.view(-1)\n",
    "            )\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:23:36.194056Z",
     "iopub.status.busy": "2024-12-15T12:23:36.193454Z",
     "iopub.status.idle": "2024-12-15T12:23:36.203257Z",
     "shell.execute_reply": "2024-12-15T12:23:36.202218Z",
     "shell.execute_reply.started": "2024-12-15T12:23:36.194015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "          print_every=100, plot_every=100, patience=5):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_train_loss_total = 0  \n",
    "    print_val_loss_total = 0  \n",
    "    \n",
    "    plot_loss_total = 0 \n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')  \n",
    "    epochs_without_improvement = 0  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_train_loss_total += train_loss\n",
    "        val_loss = val_epoch(val_dataloader, encoder, decoder, criterion)\n",
    "        print_val_loss_total += val_loss\n",
    "        plot_loss_total += train_loss\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0 \n",
    "            save_encode_path = \"encoder.pth\"\n",
    "            save_decode_path = \"decoder.pth\"\n",
    "            save_model(encoder, save_encode_path)\n",
    "            save_model(decoder, save_decode_path)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "            break\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_train_loss_avg = print_train_loss_total / print_every\n",
    "            print_train_loss_total = 0\n",
    "            print('%s (%d %d%%) Train Loss: %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                                     epoch, epoch / n_epochs * 100, print_train_loss_avg))\n",
    "            print_val_loss_avg = print_val_loss_total / print_every\n",
    "            print_val_loss_total = 0\n",
    "            print('%s (%d %d%%) Val Loss: %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                                   epoch, epoch / n_epochs * 100, print_val_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:23:36.363095Z",
     "iopub.status.busy": "2024-12-15T12:23:36.362709Z",
     "iopub.status.idle": "2024-12-15T12:23:36.368844Z",
     "shell.execute_reply": "2024-12-15T12:23:36.367957Z",
     "shell.execute_reply.started": "2024-12-15T12:23:36.363064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T12:23:38.500811Z",
     "iopub.status.busy": "2024-12-15T12:23:38.500445Z",
     "iopub.status.idle": "2024-12-15T13:26:45.435420Z",
     "shell.execute_reply": "2024-12-15T13:26:45.433062Z",
     "shell.execute_reply.started": "2024-12-15T12:23:38.500778Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18m 1s (- 207m 20s) (2 8%) Train Loss: 1.9037\n",
      "18m 1s (- 207m 20s) (2 8%) Val Loss: 1.7591\n",
      "36m 4s (- 189m 24s) (4 16%) Train Loss: 2.6265\n",
      "36m 4s (- 189m 24s) (4 16%) Val Loss: 3.5604\n",
      "54m 4s (- 171m 14s) (6 24%) Train Loss: 2.9599\n",
      "54m 4s (- 171m 14s) (6 24%) Val Loss: 3.0734\n",
      "Early stopping triggered after 7 epochs.\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "batch_size = 32\n",
    "\n",
    "encoder = EncoderRNN(VOCAB_SIZE, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, VOCAB_SIZE).to(device)\n",
    "\n",
    "train(train_dataloader,val_dataloader, encoder, decoder, 15, print_every=1, plot_every=2, patience = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T13:27:35.387650Z",
     "iopub.status.busy": "2024-12-15T13:27:35.387303Z",
     "iopub.status.idle": "2024-12-15T13:27:35.392349Z",
     "shell.execute_reply": "2024-12-15T13:27:35.391418Z",
     "shell.execute_reply.started": "2024-12-15T13:27:35.387620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T13:27:35.560309Z",
     "iopub.status.busy": "2024-12-15T13:27:35.559922Z",
     "iopub.status.idle": "2024-12-15T13:27:36.176886Z",
     "shell.execute_reply": "2024-12-15T13:27:36.175915Z",
     "shell.execute_reply.started": "2024-12-15T13:27:35.560278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_encode_path = \"encoder.pth\"\n",
    "save_decode_path = \"decoder.pth\"\n",
    "save_model(encoder, save_encode_path)\n",
    "save_model(decoder, save_decode_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T13:27:36.179201Z",
     "iopub.status.busy": "2024-12-15T13:27:36.178875Z",
     "iopub.status.idle": "2024-12-15T13:27:36.188164Z",
     "shell.execute_reply": "2024-12-15T13:27:36.187244Z",
     "shell.execute_reply.started": "2024-12-15T13:27:36.179172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def beam_search_decode(encoder, decoder, input_tensor, max_length, beam_width, sos_token, eos_token, device):\n",
    "\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    batch_size = input_tensor.size(0)\n",
    "\n",
    "    encoder_mask = (input_tensor != 1).to(device)\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "    decoder_input = torch.tensor([[sos_token]] * batch_size, dtype=torch.long, device=device)\n",
    "    decoder_hidden = decoder.transform_bidirectional_hidden(encoder_hidden)\n",
    "\n",
    "    beams = [(decoder_input, 0, decoder_hidden)]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        new_beams = []\n",
    "        for seq, score, hidden in beams:\n",
    "            decoder_output, hidden, _ = decoder.forward_step(seq[:, -1:], hidden, encoder_outputs)\n",
    "\n",
    "            topv, topi = decoder_output.squeeze(1).topk(beam_width)\n",
    "\n",
    "            for i in range(beam_width):\n",
    "                next_seq = torch.cat([seq, topi[:, i].unsqueeze(1)], dim=1)\n",
    "\n",
    "                next_score = score + topv[:, i].item()\n",
    "\n",
    "                new_beams.append((next_seq, next_score, hidden))\n",
    "\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "    best_sequence = beams[0][0]\n",
    "    return best_sequence\n",
    "\n",
    "def evaluate_with_beam_search(translator, input_tensor, max_length, beam_width, sos_token, eos_token, device):\n",
    "\n",
    "    encoder = translator.encoder\n",
    "    decoder = translator.decoder\n",
    "\n",
    "    with torch.no_grad():\n",
    "        best_sequence = beam_search_decode(encoder, decoder, input_tensor, max_length, beam_width, sos_token, eos_token, device)\n",
    "\n",
    "    return best_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T13:27:38.638666Z",
     "iopub.status.busy": "2024-12-15T13:27:38.638346Z",
     "iopub.status.idle": "2024-12-15T13:27:38.644554Z",
     "shell.execute_reply": "2024-12-15T13:27:38.643681Z",
     "shell.execute_reply.started": "2024-12-15T13:27:38.638638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_decoded_sentence(sentence):\n",
    "    special_tokens = [\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"]\n",
    "    for token in special_tokens:\n",
    "        sentence = sentence.replace(token, \"\").strip() \n",
    "    return sentence\n",
    "\n",
    "def translate_english_to_vietnamese(model, english_tokenizer, vietnamese_tokenizer, device):\n",
    "    model.eval()\n",
    "    \n",
    "    english_sentence = input(\"Enter an English sentence: \").strip()\n",
    "    \n",
    "    english_tokens = english_tokenizer.encode(english_sentence)  # Convert sentence to token IDs\n",
    "    english_tensor = torch.tensor(english_tokens).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    best_sentence = evaluate_with_beam_search(translator, english_tensor, 50, 5, 0, 2, device)\n",
    "    \n",
    "    vietnamese_sentence = vietnamese_tokenizer.decode(best_sentence[0])\n",
    "    \n",
    "    vietnamese_sentence_cleaned = clean_decoded_sentence(vietnamese_sentence)\n",
    "    \n",
    "    return vietnamese_sentence_cleaned\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6279240,
     "sourceId": 10167921,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6301675,
     "sourceId": 10198386,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6303106,
     "sourceId": 10200225,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6307250,
     "sourceId": 10206078,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
