{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10182300,"sourceType":"datasetVersion","datasetId":6289982},{"sourceId":10183431,"sourceType":"datasetVersion","datasetId":6290812},{"sourceId":10183444,"sourceType":"datasetVersion","datasetId":6290824},{"sourceId":10193937,"sourceType":"datasetVersion","datasetId":6298669}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets evaluate accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:37:36.704806Z","iopub.execute_input":"2024-12-13T21:37:36.705166Z","iopub.status.idle":"2024-12-13T21:37:46.687121Z","shell.execute_reply.started":"2024-12-13T21:37:36.705127Z","shell.execute_reply":"2024-12-13T21:37:46.686037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:37:46.689084Z","iopub.execute_input":"2024-12-13T21:37:46.689340Z","iopub.status.idle":"2024-12-13T21:37:56.992711Z","shell.execute_reply.started":"2024-12-13T21:37:46.689317Z","shell.execute_reply":"2024-12-13T21:37:56.991637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:37:56.994246Z","iopub.execute_input":"2024-12-13T21:37:56.994621Z","iopub.status.idle":"2024-12-13T21:38:05.324426Z","shell.execute_reply.started":"2024-12-13T21:37:56.994584Z","shell.execute_reply":"2024-12-13T21:38:05.323333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import os\n\ndef read_data(root, folder):\n    data = {'en':[], 'vi':[]}\n    path = os.path.join(root, folder)\n    for file_name in os.listdir(path):\n        file_path = os.path.join(path, file_name)\n        with open(file_path,'r') as f:\n            _, tail = file_path.split('.')\n            if tail =='en':\n                for line in f:\n                    data['en'].append(line.strip())\n            else:\n                for line in f:\n                    data['vi'].append(line.strip())\n                    \n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:52:18.799859Z","iopub.execute_input":"2024-12-13T21:52:18.800650Z","iopub.status.idle":"2024-12-13T21:52:18.806117Z","shell.execute_reply.started":"2024-12-13T21:52:18.800606Z","shell.execute_reply":"2024-12-13T21:52:18.805134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root = \"/kaggle/input/\"\n\ntrain_data = read_data(root, 'trainnew')\ntest_data = read_data(root, 'testnew')\nvalid_data = read_data(root, 'validd')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:52:20.882471Z","iopub.execute_input":"2024-12-13T21:52:20.882828Z","iopub.status.idle":"2024-12-13T21:52:22.040865Z","shell.execute_reply.started":"2024-12-13T21:52:20.882797Z","shell.execute_reply":"2024-12-13T21:52:22.040141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\ntrain_dataset = Dataset.from_dict(train_data)\ntest_dataset = Dataset.from_dict(test_data)\nvalid_dataset = Dataset.from_dict(valid_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:52:24.520362Z","iopub.execute_input":"2024-12-13T21:52:24.520743Z","iopub.status.idle":"2024-12-13T21:52:26.578069Z","shell.execute_reply.started":"2024-12-13T21:52:24.520709Z","shell.execute_reply":"2024-12-13T21:52:26.577097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import DatasetDict\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"test\": test_dataset,\n    \"valid\": valid_dataset\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:52:29.760443Z","iopub.execute_input":"2024-12-13T21:52:29.761311Z","iopub.status.idle":"2024-12-13T21:52:29.765317Z","shell.execute_reply.started":"2024-12-13T21:52:29.761275Z","shell.execute_reply":"2024-12-13T21:52:29.764196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:52:31.271052Z","iopub.execute_input":"2024-12-13T21:52:31.271383Z","iopub.status.idle":"2024-12-13T21:52:31.277754Z","shell.execute_reply.started":"2024-12-13T21:52:31.271352Z","shell.execute_reply":"2024-12-13T21:52:31.276745Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, EncoderDecoderModel, AutoModel\n\n# Define encoder and decoder model names\nencoder_model_name = \"bert-base-uncased\"  \ndecoder_model_name = \"vinai/bartpho-word\"  \n\n# Load tokenizers\nencoder_tokenizer = AutoTokenizer.from_pretrained(encoder_model_name)\ndecoder_tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:38:08.313836Z","iopub.execute_input":"2024-12-13T21:38:08.314522Z","iopub.status.idle":"2024-12-13T21:38:26.919354Z","shell.execute_reply.started":"2024-12-13T21:38:08.314485Z","shell.execute_reply":"2024-12-13T21:38:26.918368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_max_length=64\ndecoder_max_length=64\n\ndef process_data_to_model_inputs(batch):\n    inputs = encoder_tokenizer(batch[\"en\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n    outputs = decoder_tokenizer(batch[\"vi\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n\n    batch[\"input_ids\"] = inputs.input_ids\n    batch[\"attention_mask\"] = inputs.attention_mask\n    batch[\"labels\"] = [\n        [-100 if token == decoder_tokenizer.pad_token_id else token for token in labels]\n        for labels in outputs.input_ids\n    ]\n    return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:38:26.923090Z","iopub.execute_input":"2024-12-13T21:38:26.923619Z","iopub.status.idle":"2024-12-13T21:38:26.930817Z","shell.execute_reply.started":"2024-12-13T21:38:26.923589Z","shell.execute_reply":"2024-12-13T21:38:26.929868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size=128\n\ntokenizer_dataset = dataset.map(\n    process_data_to_model_inputs, \n    batched=True, \n    batch_size=batch_size,\n    remove_columns=[\"en\", \"vi\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:38:26.931820Z","iopub.execute_input":"2024-12-13T21:38:26.932073Z","iopub.status.idle":"2024-12-13T21:39:34.846271Z","shell.execute_reply.started":"2024-12-13T21:38:26.932049Z","shell.execute_reply":"2024-12-13T21:39:34.845427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:39:34.847537Z","iopub.execute_input":"2024-12-13T21:39:34.847901Z","iopub.status.idle":"2024-12-13T21:39:34.854452Z","shell.execute_reply.started":"2024-12-13T21:39:34.847861Z","shell.execute_reply":"2024-12-13T21:39:34.853536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metric","metadata":{}},{"cell_type":"code","source":"import evaluate\nrouge = evaluate.load(\"rouge\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:39:34.855519Z","iopub.execute_input":"2024-12-13T21:39:34.855800Z","iopub.status.idle":"2024-12-13T21:39:36.350315Z","shell.execute_reply.started":"2024-12-13T21:39:34.855772Z","shell.execute_reply":"2024-12-13T21:39:36.349576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    # Decode predictions and labels\n    pred_str = decoder_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = decoder_tokenizer.pad_token_id\n    label_str = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n\n    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])\n    rouge2_score = rouge_output[\"rouge2\"]\n\n    return {\n        \"rouge2_precision\": round(rouge2_score, 4),\n        \"rouge2_recall\": round(rouge2_score, 4),\n        \"rouge2_fmeasure\": round(rouge2_score, 4),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:39:36.351331Z","iopub.execute_input":"2024-12-13T21:39:36.351864Z","iopub.status.idle":"2024-12-13T21:39:36.357648Z","shell.execute_reply.started":"2024-12-13T21:39:36.351836Z","shell.execute_reply":"2024-12-13T21:39:36.356794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# Combine into an EncoderDecoderModel\nmodel = EncoderDecoderModel.from_encoder_decoder_pretrained(\n    encoder_model_name,\n    decoder_model_name\n)\n\nmodel.config.decoder_start_token_id = decoder_tokenizer.cls_token_id\nmodel.config.pad_token_id = decoder_tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:39:36.358766Z","iopub.execute_input":"2024-12-13T21:39:36.359105Z","iopub.status.idle":"2024-12-13T21:39:47.379747Z","shell.execute_reply.started":"2024-12-13T21:39:36.359064Z","shell.execute_reply":"2024-12-13T21:39:47.379019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your parallel dataset\ntrain_dataset = tokenizer_dataset['train']\nval_dataset = tokenizer_dataset['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:39:47.380825Z","iopub.execute_input":"2024-12-13T21:39:47.381107Z","iopub.status.idle":"2024-12-13T21:39:47.385176Z","shell.execute_reply.started":"2024-12-13T21:39:47.381081Z","shell.execute_reply":"2024-12-13T21:39:47.384265Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n    learning_rate=5e-5,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=64,\n    num_train_epochs = 5,  # Set a maximum number of epochs\n    save_total_limit=2,\n    predict_with_generate=True,\n    report_to=\"none\",\n    metric_for_best_model=\"eval_loss\",  # Specify the metric to monitor\n    greater_is_better=False,  # Smaller loss is better\n    load_best_model_at_end=True,  # Load the best model at the end of training\n    save_strategy=\"epoch\"  # Save the model at the end of each epoch\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience=2  # Stop training if no improvement for 3 consecutive evaluations\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[early_stopping_callback]  # Add the early stopping callback\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T21:39:53.680136Z","iopub.execute_input":"2024-12-13T21:39:53.680443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"./translation_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = encoder_tokenizer(\n    \"On Sunday, September 1, 2019, Hurricane Dorian, one of the strongest hurricanes ever recorded in the Atlantic Ocean, with winds of 362 km/h, made landfall on Great Abaco Island, northern Bahamas.\", \n    return_tensors=\"pt\", \n    padding=True, \n    truncation=True, \n    max_length=64\n)\n\n# Move inputs to the model's device\ninputs = {key: value.to(model.device) for key, value in inputs.items()}\n\n# Generate outputs\noutputs = model.generate(inputs[\"input_ids\"], max_length=64, num_beams=4)\n\n# Decode the output\nprint(decoder_tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset['test']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n# Function to evaluate model predictions on the test set\ndef evaluate_model(model, encoder_tokenizer, decoder_tokenizer, test_data):\n    predictions = []\n    references = []\n    \n    for item in test_data:\n        source = item[\"en\"]\n        target = item[\"vi\"]\n        \n        # Tokenize the source sentence\n        inputs = encoder_tokenizer(source, \n                                    padding=True, \n                                    truncation=True, \n                                    max_length=64,\n                                    return_tensors=\"pt\")\n        inputs = {key: value.to(model.device) for key, value in inputs.items()}\n        # Generate translation\n        outputs = model.generate(inputs[\"input_ids\"], max_length=64, num_beams=4)\n        prediction = decoder_tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        predictions.append(prediction)\n        references.append(target)\n        \n    \n    return predictions, references\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"small_test_dataset =dataset[\"test\"].shuffle(seed=42).select(range(10))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Define a smoothing function\nsmoothing = SmoothingFunction().method1\n\n# Calculate BLEU score with smoothing\ndef calculate_bleu(predictions, references):\n    scores = []\n    for pred, ref in zip(predictions, references):\n        ref_tokens = ref.split()\n        pred_tokens = pred.split()\n        # Add smoothing to handle cases with no 4-gram overlaps\n        score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)\n        scores.append(score)\n    return sum(scores) / len(scores)  # Average BLEU score\n\n# Example usage\nbleu_score = calculate_bleu(predictions, references)\nprint(f\"BLEU Score with Smoothing: {bleu_score:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}