{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10200339,"sourceType":"datasetVersion","datasetId":6303181},{"sourceId":198072,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":168929,"modelId":191281}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import os\n\nroot = \"/kaggle/input/\"\ndef read_data(root, folder):\n    data = {'en':[], 'vi':[]}\n    path = os.path.join(root, folder)\n    for file_name in os.listdir(path):\n        file_path = os.path.join(path, file_name)\n        with open(file_path,'r') as f:\n            _, tail = file_path.split('.')\n            if tail =='en':\n                for line in f:\n                    data['en'].append(line.strip())\n            else:\n                for line in f:\n                    data['vi'].append(line.strip())\n                    \n    return data\n\ntest_data = read_data(root, 'datatest')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:21:43.275305Z","iopub.execute_input":"2024-12-15T04:21:43.276260Z","iopub.status.idle":"2024-12-15T04:21:43.319617Z","shell.execute_reply.started":"2024-12-15T04:21:43.276209Z","shell.execute_reply":"2024-12-15T04:21:43.318800Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from datasets import Dataset\n\ntest_dataset = Dataset.from_dict(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:21:45.562845Z","iopub.execute_input":"2024-12-15T04:21:45.563204Z","iopub.status.idle":"2024-12-15T04:21:46.767869Z","shell.execute_reply.started":"2024-12-15T04:21:45.563172Z","shell.execute_reply":"2024-12-15T04:21:46.766946Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:21:46.769371Z","iopub.execute_input":"2024-12-15T04:21:46.769715Z","iopub.status.idle":"2024-12-15T04:21:46.776282Z","shell.execute_reply.started":"2024-12-15T04:21:46.769678Z","shell.execute_reply":"2024-12-15T04:21:46.775247Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['en', 'vi'],\n    num_rows: 100\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Get_prediction","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:21:55.202450Z","iopub.execute_input":"2024-12-15T04:21:55.202794Z","iopub.status.idle":"2024-12-15T04:21:57.995205Z","shell.execute_reply.started":"2024-12-15T04:21:55.202763Z","shell.execute_reply":"2024-12-15T04:21:57.994311Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## EncoderDecoder model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\n# Tải model cụ thể về thư mục cục bộ\nsnapshot_download(\n    repo_id=\"Sag1012/machine-translation\",  # ID repo trên Hugging Face\n    repo_type=\"model\",                  \n    cache_dir=\"/kaggle/working\",    # Đường dẫn lưu trữ cục bộ\n    allow_patterns=[\"EncoderDecoder_7/**\"]  # Tải chỉ các file mô hình\n)\n\nprint(\"Folder 'folder_name' downloaded to /kaggle/working/model/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, EncoderDecoderModel, AutoModel\n\ndef load_model_Bert_BARTPho(model_path):\n    encoder_model_name = \"bert-base-uncased\"  \n    decoder_model_name = \"vinai/bartpho-word\"  \n    \n    encoder = AutoTokenizer.from_pretrained(encoder_model_name)\n    decoder = AutoTokenizer.from_pretrained(decoder_model_name)\n    model = EncoderDecoderModel.from_pretrained(model_path)\n    return encoder, decoder, model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_predict_EncoderDecoder(model_path, test_data):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    encoder, decoder, model = load_model_Bert_BARTPho(model_path)\n    model = model.to(device)\n    \n    texts = []\n    predictions = []\n    references = []\n    \n    for item in test_data:\n        source = item[\"en\"]\n        target = item[\"vi\"]\n        \n        inputs = encoder(source, \n                        padding=True, \n                        truncation=True, \n                        max_length=len(target.split()),\n                        return_tensors=\"pt\")\n        inputs = {key: value.to(model.device) for key, value in inputs.items()}\n            # Generate translation\n        outputs = model.generate(inputs[\"input_ids\"], max_length=64, num_beams=4)\n        prediction = decoder.decode(outputs[0], skip_special_tokens=True)\n\n        texts.append(source)\n        predictions.append(prediction)\n        references.append(target)\n        \n    \n    return texts, predictions, references    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## T5","metadata":{}},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Đặt token API từ Hugging Face\nlogin(token=\"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\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\n# Tải model cụ thể về thư mục cục bộ\nsnapshot_download(\n    repo_id=\"Sag1012/machine-translation\",  # ID repo trên Hugging Face\n    repo_type=\"model\",                  \n    cache_dir=\"/kaggle/working\",    # Đường dẫn lưu trữ cục bộ\n    allow_patterns=[\"T5/**\"]  # Tải chỉ các file mô hình\n)\n\nprint(\"Folder 'folder_name' downloaded to /kaggle/working/model/\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, EncoderDecoderModel, T5Tokenizer, T5ForConditionalGeneration\nimport sentencepiece as spm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_model_T5(model_path):\n    encoder = T5Tokenizer.from_pretrained(\"t5-small\")\n    decoder = spm.SentencePieceProcessor(model_file = '/kaggle/working/models--Sag1012--machine-translation/snapshots/164bfec8e7d09d77ab222a6055293e66934994ca/T5/vi_tokenizer_32128.model')\n    model = T5ForConditionalGeneration.from_pretrained(model_path)\n    return encoder, decoder, model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_predict_T5(model_path, test_data):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    encoder, decoder, model = load_model_T5(model_path)\n    model = model.to(device)\n\n    texts = []\n    predictions = []\n    references = []\n    \n    for item in test_data:\n        source = item[\"en\"]\n        target = item[\"vi\"]\n        \n        inputs = encoder(source, \n                        padding=True, \n                        truncation=True, \n                        max_length=len(target.split()),\n                        return_tensors=\"pt\")\n        inputs = {key: value.to(model.device) for key, value in inputs.items()}\n        outputs = model.generate(inputs[\"input_ids\"], max_length=64, num_beams=4)\n        prediction = decoder.decode(outputs[0].tolist())\n            \n        texts.append(source)\n        predictions.append(prediction)\n        references.append(target)\n        \n    \n    return texts, predictions, references    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## BiLSTM","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\n# Tải model cụ thể về thư mục cục bộ\nsnapshot_download(\n    repo_id=\"Sag1012/machine-translation\",  # ID repo trên Hugging Face\n    repo_type=\"model\",                  \n    cache_dir=\"/kaggle/working/model/model\",    # Đường dẫn lưu trữ cục bộ\n    allow_patterns=[\"BiLSTM/**\"]  # Tải chỉ các file mô hình\n)\n\nprint(\"Folder 'folder_name' downloaded to /kaggle/working/model/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_predict_BiLSTM(model_path,test_data):\n    import os\n    directory_path = os.path.dirname(model_path)\n    texts = []\n    predictions = []\n    references = []\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.models import load_model\n    model = load_model(model_path)\n    encoder_input = model.input[0]  # Input tensor for the model\n    encoder_output = model.get_layer(\"bidirectional\").output[0]\n    encoder_state_h = model.get_layer(\"state_h_concat\").output\n    encoder_state_c = model.get_layer(\"state_c_concat\").output\n    \n    # Encoder inference model\n    encoder_model = Model(encoder_input, [encoder_output, encoder_state_h, encoder_state_c])\n    decoder_embedding = model.get_layer(\"decoder_embedding\")\n    decoder_lstm = model.get_layer(\"decoder_lstm\")\n    decoder_dense = model.get_layer(\"decoder_dense\")\n    from tensorflow.keras.layers import Input\n    units = 128  # LSTM units\n    # Decoder inference inputs\n    decoder_state_input_h = Input(shape=(units * 2,), name=\"decoder_state_input_h\")  # BiLSTM doubles the size\n    decoder_state_input_c = Input(shape=(units * 2,), name=\"decoder_state_input_c\")\n    \n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    \n    # Reuse the embedding and LSTM layers\n    decoder_input = Input(shape=(1,), name=\"decoder_input\")  # Decoder input for one time step\n    decoder_embedding_inf = decoder_embedding(decoder_input)\n    decoder_lstm_inf = decoder_lstm(decoder_embedding_inf, initial_state=decoder_states_inputs)\n    decoder_output_inf, state_h_inf, state_c_inf = decoder_lstm_inf\n    \n    decoder_states_inf = [state_h_inf, state_c_inf]\n    \n    # Dense layer for probabilities\n    decoder_output_inf = decoder_dense(decoder_output_inf)\n    \n    # Decoder inference model\n    decoder_model = Model(\n        [decoder_input] + decoder_states_inputs,  # Inputs\n        [decoder_output_inf] + decoder_states_inf)  # Outputs\n    import numpy as np\n    from tensorflow.keras.preprocessing.sequence import pad_sequences\n    def preprocess_sentence(sentence, tokenizer, max_length):\n        \"\"\"Preprocess and tokenize an input sentence.\"\"\"\n        sequence = tokenizer.texts_to_sequences([sentence])\n        return pad_sequences(sequence, maxlen=max_length, padding='post')\n    \n    def decode_sequence(input_seq):\n        \"\"\"Generate a Vietnamese sentence from an English input sequence.\"\"\"\n        # Encode the input sequence to get initial states\n    \n        encoder_output, state_h, state_c = encoder_model.predict(input_seq)\n    \n        # Initialize the decoder input with the <start> token\n        target_seq = np.zeros((1, 1))  # Shape: (batch_size, 1)\n        target_seq[0, 0] = vi_loaded_tokenizer.texts_to_sequences([\"<SOS>\"])[0][0]\n    \n        # Initialize states\n        states = [state_h, state_c]\n    \n        # Generate the output sequence token by token\n        decoded_sentence = []\n        for _ in range(232):\n            output_tokens, h, c = decoder_model.predict([target_seq] + states)\n    \n            # Sample the next token\n            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n            sampled_token = vi_loaded_tokenizer.index_word.get(sampled_token_index, '<unk>')\n            if sampled_token == '<eos>':\n                break\n    \n            decoded_sentence.append(sampled_token)\n    \n            # Update the target sequence (input to the decoder)\n            target_seq[0, 0] = sampled_token_index\n    \n            # Update states\n            states = [h, c]\n    \n        return ' '.join(decoded_sentence)\n    import pickle\n    with open(directory_path + '/english_tokenizer.pkl', 'rb') as file:\n        eng_loaded_tokenizer = pickle.load(file)\n    with open(directory_path +'/vietnamese_tokenizer.pkl', 'rb') as file:\n        vi_loaded_tokenizer = pickle.load(file)\n    \n\n    for item in test_data:\n        source = item[\"en\"]\n        target = item[\"vi\"]\n        \n        input_sentence = source\n        input_sequence = preprocess_sentence(input_sentence, eng_loaded_tokenizer, 193)\n        \n        prediction = decode_sequence(input_sequence)\n            \n        texts.append(source)\n        predictions.append(prediction)\n        references.append(target)\n        \n    \n    return texts, predictions, references   \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Bi-GRU with attention","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\n# Tải model cụ thể về thư mục cục bộ\nsnapshot_download(\n    repo_id=\"Sag1012/machine-translation\",  # ID repo trên Hugging Face\n    repo_type=\"model\",                  \n    cache_dir=\"/kaggle/working/model\",    # Đường dẫn lưu trữ cục bộ\n    allow_patterns=[\"GRU_with_attention ver4/**\"]  # Tải chỉ các file mô hình\n)\n\nprint(\"Folder 'folder_name' downloaded to /kaggle/working/model/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport unicodedata\nimport re\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer, AutoModel, PreTrainedTokenizerFast","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSOS_token = 0\nMAX_LENGTH = 50","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size).to(device)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True).to(device)\n        self.hidden_transform = nn.Linear(hidden_size * 2, hidden_size).to(device)\n\n    def forward(self, input):\n        input = input.to(device)\n        embedded = self.embedding(input)\n        output, hidden = self.gru(embedded)\n        output, hidden = output.to(device), hidden.to(device)\n        output = self.hidden_transform(output)\n\n        return output, hidden","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CrossAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super(CrossAttention, self).__init__()\n        self.Wa = nn.Linear(hidden_size, hidden_size).to(device)  # Linear layer for the query\n        self.Ua = nn.Linear(hidden_size, hidden_size).to(device)    # Linear layer for the keys\n        self.Va = nn.Linear(hidden_size, hidden_size).to(device)  # Linear layer for the values\n        self.softmax = nn.Softmax(dim=-1).to(device)  \n\n    def forward(self, query, keys):\n\n        query_proj = self.Wa(query).to(device)  # Shape: (batch_size, query_len, hidden_size)\n        key_proj = self.Ua(keys).to(device)      # Shape: (batch_size, key_len, hidden_size)\n        value_proj = self.Va(keys).to(device)  # Shape: (batch_size, key_len, hidden_size)\n\n        scores = torch.bmm(query_proj, key_proj.transpose(1, 2)).to(device)  # Shape: (batch_size, query_len, key_len)\n        scores = scores / torch.sqrt(torch.tensor(key_proj.size(-1), dtype=torch.float32, device=device))  # Scale by sqrt(hidden_size)\n        \n\n        # Compute attention weights\n        attention_weights = self.softmax(scores).to(device)  # Shape: (batch_size, query_len, key_len)\n\n        # Compute context vectors as weighted sum of values\n        context = torch.bmm(attention_weights, value_proj).to(device)  # Shape: (batch_size, query_len, hidden_size)\n\n        return context, attention_weights\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n        super(AttnDecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(output_size, hidden_size).to(device)\n        self.attention = CrossAttention(hidden_size).to(device)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n                          dropout=(0 if n_layers == 1 else dropout_p), batch_first=True).to(device)\n        self.out = nn.Linear(hidden_size, output_size).to(device)\n        self.dropout = nn.Dropout(dropout_p).to(device)\n        self.hidden_transform = nn.Linear(hidden_size * 2, hidden_size).to(device)\n        self.hidden_input_transform = nn.Linear(hidden_size * 2, hidden_size).to(device)\n        self.hidden_size = hidden_size\n\n    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n        encoder_outputs = encoder_outputs.to(device)\n        encoder_hidden = encoder_hidden.to(device)\n        if target_tensor is not None:\n            target_tensor = target_tensor.to(device)\n\n        batch_size = encoder_outputs.size(0)\n        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token).to(device)\n        decoder_hidden = self.transform_bidirectional_hidden(encoder_hidden)\n        decoder_outputs = []\n        attentions = []\n\n        for i in range(MAX_LENGTH):\n            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            decoder_outputs.append(decoder_output)\n            attentions.append(attn_weights)\n\n            if target_tensor is not None:\n                # Teacher forcing: Feed the target as the next input\n                decoder_input = target_tensor[:, i].unsqueeze(1).to(device)  # Teacher forcing\n            else:\n                # Without teacher forcing: use its own predictions as the next input\n                _, topi = decoder_output.topk(1)\n                decoder_input = topi.squeeze(-1).detach().to(device)  # detach from history as input\n\n        decoder_outputs = torch.cat(decoder_outputs, dim=1).to(device)\n        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1).to(device)\n        attentions = torch.cat(attentions, dim=1).to(device)\n\n        return decoder_outputs, decoder_hidden, attentions\n\n    def transform_bidirectional_hidden(self, encoder_hidden):\n        forward_states = encoder_hidden[0::2, :, :].to(device)  # Forward states: (batch, num_layers, hidden_size)\n        backward_states = encoder_hidden[1::2, :, :].to(device)  # Backward states: (batch, num_layers, hidden_size)\n        combined_hidden = torch.cat((forward_states, backward_states), dim=2).to(device)  # Shape: (batch, num_layers, hidden_size * 2)\n        combined_hidden = self.hidden_transform(combined_hidden).to(device)\n        return combined_hidden\n\n    def forward_step(self, input, hidden, encoder_outputs):\n        encoder_outputs = encoder_outputs.to(device)\n\n        embedded = self.embedding(input).to(device)\n        query = hidden.permute(1, 0, 2)\n        context, attn_weights = self.attention(query, encoder_outputs)\n        input_gru = torch.cat((embedded, context), dim=2)\n        input_gru = self.hidden_input_transform(input_gru)\n        output, hidden = self.gru(input_gru, hidden)\n        output = self.out(output)\n\n        return output, hidden, attn_weights\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Translator(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super(Translator, self).__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, input_tensor, target_tensor=None):\n        if target_tensor is not None:\n            target_tensor = target_tensor.to(self.device)\n\n        encoder_outputs, encoder_hidden = self.encoder(input_tensor)\n        decoder_outputs, _, _ = self.decoder(encoder_outputs, encoder_hidden, target_tensor)\n        return decoder_outputs\n\n    def eval(self):\n        self.encoder.eval()\n        self.decoder.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_model_GRU(model, path):\n    model.load_state_dict(torch.load(path))\n    model.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_decoded_sentence(sentence):\n    special_tokens = [\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"]\n    for token in special_tokens:\n        sentence = sentence.replace(token, \"\").strip() \n    return sentence","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate_GRU(input_text):\n    encoder_pth = \"/kaggle/working/model/models--Sag1012--machine-translation/snapshots/c5f85377fb64307c86c935bae0edf64b764d8db8/GRU_with_attention ver3/encoder.pth\"\n    decoder_pth = \"/kaggle/working/model/models--Sag1012--machine-translation/snapshots/c5f85377fb64307c86c935bae0edf64b764d8db8/GRU_with_attention ver3/decoder.pth\"\n    VOCAB_SIZE = 64000\n    hidden_size = 256\n    encoder = EncoderRNN(VOCAB_SIZE, hidden_size)\n    decoder = AttnDecoderRNN(hidden_size, VOCAB_SIZE)\n    load_model_GRU(encoder, encoder_pth)\n    load_model_GRU(decoder, decoder_pth)\n    translator = Translator(encoder,decoder,device)\n    english_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    vietnamese_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n    english_tokens = english_tokenizer.encode(input_text)\n    english_tensor = torch.tensor(english_tokens).unsqueeze(0).to(device)\n    print(device)\n    with torch.no_grad():\n        output_tensor = translator(english_tensor)\n    predicted_token_ids = torch.argmax(output_tensor, dim=-1).squeeze(0).tolist()\n    vietnamese_sentence = vietnamese_tokenizer.decode(predicted_token_ids)\n    vietnamese_sentence_cleaned = clean_decoded_sentence(vietnamese_sentence)\n    \n    return vietnamese_sentence_cleaned\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_predict_GRU(model_path,test_data):\n    import os\n    directory_path = os.path.dirname(model_path)\n    encoder_pth = directory_path + \"/encoder.pth\"\n    decoder_pth = directory_path + \"/decoder.pth\"\n    \n    texts = []\n    predictions = []\n    references = []\n\n    VOCAB_SIZE = 64000\n    hidden_size = 256\n    encoder = EncoderRNN(VOCAB_SIZE, hidden_size)\n    decoder = AttnDecoderRNN(hidden_size, VOCAB_SIZE)\n    load_model_GRU(encoder, encoder_pth)\n    load_model_GRU(decoder, decoder_pth)\n    translator = Translator(encoder,decoder,device)\n    english_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    vietnamese_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\n    for item in test_data:\n        source = item[\"en\"]\n        target = item[\"vi\"]\n        \n        input_sentence = source\n        english_tokens = english_tokenizer.encode(input_sentence)\n        english_tensor = torch.tensor(english_tokens).unsqueeze(0).to(device)\n        with torch.no_grad():\n            output_tensor = translator(english_tensor)\n        predicted_token_ids = torch.argmax(output_tensor, dim=-1).squeeze(0).tolist()\n        vietnamese_sentence = vietnamese_tokenizer.decode(predicted_token_ids)\n        vietnamese_sentence_cleaned = clean_decoded_sentence(vietnamese_sentence)\n                    \n        texts.append(source)\n        predictions.append(vietnamese_sentence_cleaned)\n        references.append(target)\n    return texts, predictions, references","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Marian MT","metadata":{}},{"cell_type":"code","source":"\nfrom huggingface_hub import snapshot_download\n\n# Tải model cụ thể về thư mục cục bộ\nsnapshot_download(\n    repo_id=\"Sag1012/machine-translation\",  \n    repo_type=\"model\",                  \n    cache_dir=\"/kaggle/working/model\",    # Đường dẫn lưu trữ cục bộ\n    allow_patterns=[\"MarianMT_ver2/**\"]  # Tải chỉ các file mô hình\n)\n\nprint(\"Folder 'folder_name' downloaded to /kaggle/working/model/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:22:11.307823Z","iopub.execute_input":"2024-12-15T04:22:11.308182Z","iopub.status.idle":"2024-12-15T04:22:25.294488Z","shell.execute_reply.started":"2024-12-15T04:22:11.308148Z","shell.execute_reply":"2024-12-15T04:22:25.293590Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6bf5a20d6c1400caddc4d7b070f6a62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"MarianMT_ver2/generation_config.json:   0%|          | 0.00/288 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c4386d2cd9540de9bf7d649c3cf4a9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"MarianMT_ver2/trainer_state.json:   0%|          | 0.00/1.90k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d649913f1d748298d51f63d74ca61fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"MarianMT_ver2/config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b37ff9e50e5411e8bb9851a415b329b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e47987ea8de44166ab024c48c1fdbb79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42861aab0f6d42a6b64bde6565a72c4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e6dad43901c40a69f6d6ad2c13f0a47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/287M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e67f2cdb064e4d2a95fb23bd31f8d62b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"optimizer.pt:   0%|          | 0.00/573M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37cb907bde844b68a1866c3fee5ac0df"}},"metadata":{}},{"name":"stdout","text":"Folder 'folder_name' downloaded to /kaggle/working/model/\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install sacremoses\n\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\n\ndef load_model_MarianMT(model_path):\n\n    tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-vi')\n    model = MarianMTModel.from_pretrained(model_path)\n    return tokenizer, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:29:12.755975Z","iopub.execute_input":"2024-12-15T04:29:12.756340Z","iopub.status.idle":"2024-12-15T04:29:12.761101Z","shell.execute_reply.started":"2024-12-15T04:29:12.756309Z","shell.execute_reply":"2024-12-15T04:29:12.760227Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def get_predict_MarianMT(model_path, test_data):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    tokenizer, model = load_model_MarianMT(model_path)\n    model = model.to(device)\n    \n    texts = []\n    predictions = []\n    references = []\n    \n    for item in test_data:\n        source = item[\"en\"]  # Replace \"en\" with the source language key\n        target = item[\"vi\"]  # Replace \"vi\" with the target language key\n        \n        # Tokenize input\n        inputs = tokenizer(source, return_tensors=\"pt\", padding=True, truncation=True)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        \n        # Generate translation\n        outputs = model.generate(**inputs, max_length=64, num_beams=4)\n        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        texts.append(source)\n        predictions.append(prediction)\n        references.append(target)\n    \n    return texts, predictions, references","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:29:16.193999Z","iopub.execute_input":"2024-12-15T04:29:16.194369Z","iopub.status.idle":"2024-12-15T04:29:16.201187Z","shell.execute_reply.started":"2024-12-15T04:29:16.194338Z","shell.execute_reply":"2024-12-15T04:29:16.200269Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def cos_sim(predictions, references):\n    from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n    from transformers import  AutoTokenizer, AutoModel\n    \n    # Load model and tokenizer\n    decoder_model_name = \"vinai/bartpho-word\"\n    tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\n    model = AutoModel.from_pretrained(decoder_model_name)\n\n    # Move the model to GPU if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    cos = []\n\n    # Process each prediction and reference pair\n    for pred, ref in zip(predictions, references):\n        # Tokenize predictions and references\n        p = tokenizer(pred, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n        p.pop(\"token_type_ids\", None)\n        r = tokenizer(ref, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n        r.pop(\"token_type_ids\", None)\n\n        # Generate embeddings\n        with torch.no_grad():\n            embeddings1 = model(**p).last_hidden_state.mean(dim=1)  # Prediction embeddings\n            embeddings2 = model(**r).last_hidden_state.mean(dim=1)  # Reference embeddings\n\n        # Compute cosine similarity\n        similarity = torch_cosine_similarity(embeddings1, embeddings2).item()  # Use PyTorch cosine similarity\n        cos.append(similarity)\n\n    return cos\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:29:18.352265Z","iopub.execute_input":"2024-12-15T04:29:18.352604Z","iopub.status.idle":"2024-12-15T04:29:18.359555Z","shell.execute_reply.started":"2024-12-15T04:29:18.352574Z","shell.execute_reply":"2024-12-15T04:29:18.358559Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef calculate_bleu_scores(predictions, references, weights = (1,0,0,0)):\n\n    smoothing = SmoothingFunction().method1\n    BLEU_scores = []\n    \n    for pred, ref in zip(predictions, references):\n        ref_tokens = ref.split()\n        pred_tokens = pred.split()\n\n        bleu = sentence_bleu([ref_tokens], pred_tokens, weights=weights, smoothing_function=smoothing)\n        BLEU_scores.append(bleu)\n    \n    return BLEU_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:29:20.463836Z","iopub.execute_input":"2024-12-15T04:29:20.464546Z","iopub.status.idle":"2024-12-15T04:29:20.469412Z","shell.execute_reply.started":"2024-12-15T04:29:20.464510Z","shell.execute_reply":"2024-12-15T04:29:20.468559Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:29:21.928611Z","iopub.execute_input":"2024-12-15T04:29:21.929389Z","iopub.status.idle":"2024-12-15T04:29:21.932984Z","shell.execute_reply.started":"2024-12-15T04:29:21.929354Z","shell.execute_reply":"2024-12-15T04:29:21.932157Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def evaluate_model(model_name, model_path, test_data):\n    fn = globals()[f\"get_predict_{model_name}\"]\n    texts, predictions, references = fn(model_path, test_data)\n    cos = cos_sim(predictions, references)\n    scores1 = calculate_bleu_scores(predictions, references, weights = (1,0,0,0))\n    scores2 = calculate_bleu_scores(predictions, references, weights = (0.5,0.5,0,0))\n    scores3 = calculate_bleu_scores(predictions, references, weights = (0.5,0.25,0.25,0))\n    scores4 = calculate_bleu_scores(predictions, references, weights = (0.25,0.25,0.25,0.25))\n    \n    data ={\n         'texts': texts,\n         'predictions': predictions,\n         'references': references,\n         'BLEU_1': scores1,\n         'BLEU_2': scores2,\n        'BLEU_3': scores3,\n        'BLEU_4': scores4,\n         \"cosin similarity\": cos\n     }\n        \n    \n    return pd.DataFrame(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:29:23.503897Z","iopub.execute_input":"2024-12-15T04:29:23.504662Z","iopub.status.idle":"2024-12-15T04:29:23.510571Z","shell.execute_reply.started":"2024-12-15T04:29:23.504627Z","shell.execute_reply":"2024-12-15T04:29:23.509527Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"model_name = 'MarianMT'\nmodel_path= '/kaggle/working/model/models--Sag1012--machine-translation/snapshots/dff7854613e72ee87d975bc13c01813f05dd3dc5/MarianMT_ver2'\n\ndf = evaluate_model(model_name, model_path, test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:29:25.636852Z","iopub.execute_input":"2024-12-15T04:29:25.637300Z","iopub.status.idle":"2024-12-15T04:29:54.983891Z","shell.execute_reply.started":"2024-12-15T04:29:25.637267Z","shell.execute_reply":"2024-12-15T04:29:54.982963Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"861b570563474bcba77d627898a00b55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/809k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b24d139dbe7646b0860d4215061b0d9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/756k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ab8ed6a46154c0cac0e540c9b2b4fda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.19M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9689d6afe0c4a3984a59c1ba15e1c65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bc39cccd72f4b8abe8e7bb327d57fb6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:30:16.464523Z","iopub.execute_input":"2024-12-15T04:30:16.465238Z","iopub.status.idle":"2024-12-15T04:30:16.488894Z","shell.execute_reply.started":"2024-12-15T04:30:16.465203Z","shell.execute_reply":"2024-12-15T04:30:16.488121Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                                texts  \\\n0   Murillo has served as the Nicaraguan governmen...   \n1   Work : People may regret not following a diffe...   \n2           M- my wife barely lets me see her naked .   \n3   It 's full of panic and fear , and I 'd heard ...   \n4   This will make it much easier for you to expos...   \n..                                                ...   \n95  You can click on any text box and start typing...   \n96     Yeah , they 're rocket scientists , remember ?   \n97     Fidgeting is a sign that you lack confidence .   \n98  Agreeing with the angry person might help diff...   \n99  Russell , is going to lose his lectureship as ...   \n\n                                          predictions  \\\n0   Murillo đã từng là người phát ngôn chính của c...   \n1   Công việc : Người ta có thể hối tiếc vì không ...   \n2   Vợ tôi hầu như không cho tôi thấy cô ấy trần t...   \n3   Nó đầy hoảng loạn và sợ hãi , và tôi nghe thấy...   \n4   Điều này sẽ giúp bạn dễ dàng vạch trần quả bón...   \n..                                                ...   \n95  Bạn có thể nhấp vào hộp văn bản và bắt đầu gõ ...   \n96  Phải , họ là những nhà khoa học tên lửa , nhớ ...   \n97   Tán tỉnh là dấu hiệu cho thấy bạn thiếu tự tin .   \n98  Đồng ý với người có tính nóng giận có thể giúp...   \n99  Russell , sẽ mất chức giảng viên do kết quả củ...   \n\n                                           references    BLEU_1    BLEU_2  \\\n0   Murillo đã từng là phát ngôn viên chính của ch...  0.817283  0.716367   \n1   Công việc : Con người có thể cảm thấy hối tiếc...  0.641245  0.494335   \n2   Vợ ... vợ tôi còn hiếm khi để tôi nhìn cô ấy k...  0.395725  0.168151   \n3   Nó chứa đầy sợ hãi và hoảng loạn và tôi nghe t...  0.680000  0.504975   \n4   Bước này sẽ giúp bạn dễ dàng làm lộ rễ cây để ...  0.545455  0.412861   \n..                                                ...       ...       ...   \n95  Bạn có thể nhấp chuột vào bất kỳ hộp thoại nào...  0.619198  0.433439   \n96   Ừ , họ là các nhà khoa học tên lửa , nhớ không ?  0.857143  0.811998   \n97  Đó là dấu hiệu cho thấy sự thiếu tự tin của bạn .  0.766704  0.620294   \n98  Việc bày tỏ sự đồng tình với người đang tức gi...  0.378733  0.259808   \n99  Russell , sẽ mất quyền giảng dạy vì cái vụ phá...  0.500000  0.324443   \n\n      BLEU_3    BLEU_4  cosin similarity  \n0   0.668703  0.556204          0.969939  \n1   0.436449  0.297307          0.921119  \n2   0.096638  0.037076          0.717559  \n3   0.387803  0.110886          0.898981  \n4   0.366260  0.257534          0.841695  \n..       ...       ...               ...  \n95  0.403613  0.280517          0.850169  \n96  0.783462  0.699752          0.915851  \n97  0.574014  0.412491          0.807538  \n98  0.221808  0.134033          0.766936  \n99  0.276536  0.161956          0.777876  \n\n[100 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>texts</th>\n      <th>predictions</th>\n      <th>references</th>\n      <th>BLEU_1</th>\n      <th>BLEU_2</th>\n      <th>BLEU_3</th>\n      <th>BLEU_4</th>\n      <th>cosin similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Murillo has served as the Nicaraguan governmen...</td>\n      <td>Murillo đã từng là người phát ngôn chính của c...</td>\n      <td>Murillo đã từng là phát ngôn viên chính của ch...</td>\n      <td>0.817283</td>\n      <td>0.716367</td>\n      <td>0.668703</td>\n      <td>0.556204</td>\n      <td>0.969939</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Work : People may regret not following a diffe...</td>\n      <td>Công việc : Người ta có thể hối tiếc vì không ...</td>\n      <td>Công việc : Con người có thể cảm thấy hối tiếc...</td>\n      <td>0.641245</td>\n      <td>0.494335</td>\n      <td>0.436449</td>\n      <td>0.297307</td>\n      <td>0.921119</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>M- my wife barely lets me see her naked .</td>\n      <td>Vợ tôi hầu như không cho tôi thấy cô ấy trần t...</td>\n      <td>Vợ ... vợ tôi còn hiếm khi để tôi nhìn cô ấy k...</td>\n      <td>0.395725</td>\n      <td>0.168151</td>\n      <td>0.096638</td>\n      <td>0.037076</td>\n      <td>0.717559</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>It 's full of panic and fear , and I 'd heard ...</td>\n      <td>Nó đầy hoảng loạn và sợ hãi , và tôi nghe thấy...</td>\n      <td>Nó chứa đầy sợ hãi và hoảng loạn và tôi nghe t...</td>\n      <td>0.680000</td>\n      <td>0.504975</td>\n      <td>0.387803</td>\n      <td>0.110886</td>\n      <td>0.898981</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>This will make it much easier for you to expos...</td>\n      <td>Điều này sẽ giúp bạn dễ dàng vạch trần quả bón...</td>\n      <td>Bước này sẽ giúp bạn dễ dàng làm lộ rễ cây để ...</td>\n      <td>0.545455</td>\n      <td>0.412861</td>\n      <td>0.366260</td>\n      <td>0.257534</td>\n      <td>0.841695</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>You can click on any text box and start typing...</td>\n      <td>Bạn có thể nhấp vào hộp văn bản và bắt đầu gõ ...</td>\n      <td>Bạn có thể nhấp chuột vào bất kỳ hộp thoại nào...</td>\n      <td>0.619198</td>\n      <td>0.433439</td>\n      <td>0.403613</td>\n      <td>0.280517</td>\n      <td>0.850169</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>Yeah , they 're rocket scientists , remember ?</td>\n      <td>Phải , họ là những nhà khoa học tên lửa , nhớ ...</td>\n      <td>Ừ , họ là các nhà khoa học tên lửa , nhớ không ?</td>\n      <td>0.857143</td>\n      <td>0.811998</td>\n      <td>0.783462</td>\n      <td>0.699752</td>\n      <td>0.915851</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>Fidgeting is a sign that you lack confidence .</td>\n      <td>Tán tỉnh là dấu hiệu cho thấy bạn thiếu tự tin .</td>\n      <td>Đó là dấu hiệu cho thấy sự thiếu tự tin của bạn .</td>\n      <td>0.766704</td>\n      <td>0.620294</td>\n      <td>0.574014</td>\n      <td>0.412491</td>\n      <td>0.807538</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>Agreeing with the angry person might help diff...</td>\n      <td>Đồng ý với người có tính nóng giận có thể giúp...</td>\n      <td>Việc bày tỏ sự đồng tình với người đang tức gi...</td>\n      <td>0.378733</td>\n      <td>0.259808</td>\n      <td>0.221808</td>\n      <td>0.134033</td>\n      <td>0.766936</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>Russell , is going to lose his lectureship as ...</td>\n      <td>Russell , sẽ mất chức giảng viên do kết quả củ...</td>\n      <td>Russell , sẽ mất quyền giảng dạy vì cái vụ phá...</td>\n      <td>0.500000</td>\n      <td>0.324443</td>\n      <td>0.276536</td>\n      <td>0.161956</td>\n      <td>0.777876</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 8 columns</p>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"df['BLEU_1'].sum()/len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:30:25.244322Z","iopub.execute_input":"2024-12-15T04:30:25.244639Z","iopub.status.idle":"2024-12-15T04:30:25.251588Z","shell.execute_reply.started":"2024-12-15T04:30:25.244613Z","shell.execute_reply":"2024-12-15T04:30:25.250611Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"0.6070899981850879"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"df['BLEU_2'].sum()/len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:30:26.849896Z","iopub.execute_input":"2024-12-15T04:30:26.850255Z","iopub.status.idle":"2024-12-15T04:30:26.856841Z","shell.execute_reply.started":"2024-12-15T04:30:26.850222Z","shell.execute_reply":"2024-12-15T04:30:26.855823Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"0.4849884020116053"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"df['BLEU_3'].sum()/len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:30:28.515574Z","iopub.execute_input":"2024-12-15T04:30:28.515924Z","iopub.status.idle":"2024-12-15T04:30:28.522414Z","shell.execute_reply.started":"2024-12-15T04:30:28.515891Z","shell.execute_reply":"2024-12-15T04:30:28.521474Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0.43604474040861013"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"df['BLEU_4'].sum()/len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:30:30.174274Z","iopub.execute_input":"2024-12-15T04:30:30.175185Z","iopub.status.idle":"2024-12-15T04:30:30.183565Z","shell.execute_reply.started":"2024-12-15T04:30:30.175127Z","shell.execute_reply":"2024-12-15T04:30:30.182121Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"0.33090962475722635"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"df['cosin similarity'].sum()/len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:30:32.673743Z","iopub.execute_input":"2024-12-15T04:30:32.674096Z","iopub.status.idle":"2024-12-15T04:30:32.680527Z","shell.execute_reply.started":"2024-12-15T04:30:32.674052Z","shell.execute_reply":"2024-12-15T04:30:32.679550Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0.8441271716356278"},"metadata":{}}],"execution_count":30}]}