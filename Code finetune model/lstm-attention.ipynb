{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import  AutoTokenizer, AutoModel\n",
    "from tokenizers import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, MultiHeadAttention, LayerNormalization, Add, Dense, Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vi_sentences_path = \"/kaggle/input/berttokenize/Bert/tokenize_vi.txt\" # change at will\n",
    "en_sentences_path = \"/kaggle/input/berttokenize/Bert/tokenize_en.txt\" # change at will\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_vi = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vi_vocab_size = tokenizer_vi.vocab_size\n",
    "en_vocab_size = tokenizer_en.vocab_size\n",
    "\n",
    "print(f\"Vietnamese Vocabulary Size: {vi_vocab_size}\")\n",
    "print(f\"English Vocabulary Size: {en_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_text = \"This is an English sentence\"\n",
    "\n",
    "input_ids = tokenizer_en.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "decoded_text = tokenizer_en.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_text = \"Với bài toán dịch Anh - Việt, việc kiểm tra cách mà tokenizer mã hóa câu tiếng Anh và tái mã hóa lại câu tiếng Việt là rất quan trọng. Dưới đây là hướng dẫn cụ thể\"\n",
    "\n",
    "# Tokenize câu\n",
    "input_ids = tokenizer_vi.encode(input_text)\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "decoded_text = tokenizer_vi.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return len(lines)\n",
    "\n",
    "# Example: Count the number of sentences in the tokenized_vi.txt file\n",
    "\n",
    "num_sentences_vi = count_sentences(vi_sentences_path)\n",
    "num_sentences_en = count_sentences(en_sentences_path)\n",
    "\n",
    "print(f\"Number of sentences in tokenized vietnamese: {num_sentences_vi}\")\n",
    "print(f\"Number of sentences in tokenized english: {num_sentences_en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the vocabulary for both tokenizers\n",
    "vi_vocab = tokenizer_vi.get_vocab()  \n",
    "en_vocab = tokenizer_en.get_vocab()  \n",
    "\n",
    "print(\"First 20 tokens in the English vocabulary:\")\n",
    "for i, (token, _) in enumerate(list(en_vocab.items())[:20]):\n",
    "    print(f\"{i+1}. {token}\")\n",
    "\n",
    "print(\"\\nFirst 20 tokens in the Vietnamese vocabulary:\")\n",
    "for i, (token, _) in enumerate(list(vi_vocab.items())[:20]):\n",
    "    print(f\"{i+1}. {token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read tokenized sentences\n",
    "def read_tokenized_sentences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "    return [list(map(int, line.strip().split())) for line in lines]\n",
    "\n",
    "# Load tokenized sentences\n",
    "tokenized_en = read_tokenized_sentences(en_sentences_path)\n",
    "tokenized_vi = read_tokenized_sentences(vi_sentences_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Take only 1% of the data\n",
    "def sample_data(english_sentences, vietnamese_sentences, sample_ratio=0.02):\n",
    "    dataset_size = len(english_sentences)\n",
    "    sample_size = int(sample_ratio * dataset_size)\n",
    "    indices = np.random.choice(dataset_size, sample_size, replace=False)\n",
    "\n",
    "    sampled_en = [english_sentences[i] for i in indices]\n",
    "    sampled_vi = [vietnamese_sentences[i] for i in indices]\n",
    "\n",
    "    return sampled_en, sampled_vi\n",
    "\n",
    "sampled_en, sampled_vi = sample_data(tokenized_en, tokenized_vi,sample_ratio=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_tf_dataset(english_sentences, vietnamese_sentences, train_split=0.9):\n",
    "    dataset_size = len(english_sentences)\n",
    "    indices = np.arange(dataset_size)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_size = int(train_split * dataset_size)\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "\n",
    "    def select_data(indices):\n",
    "        en_data = tf.constant([english_sentences[i] for i in indices], dtype=tf.int32)\n",
    "        vi_data = tf.constant([vietnamese_sentences[i] for i in indices], dtype=tf.int32)\n",
    "        return tf.data.Dataset.from_tensor_slices((en_data, vi_data))\n",
    "\n",
    "    train_data = select_data(train_indices)\n",
    "    val_data = select_data(val_indices)\n",
    "\n",
    "    return train_data, val_data\n",
    "    \n",
    "train_data, val_data = create_tf_dataset(sampled_en, sampled_vi, train_split=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_dynamic_parallel(dataset):\n",
    "    def map_func(english, vietnamese):\n",
    "        # Split Vietnamese target into input (targ_in) and output (targ_out)\n",
    "        targ_in = vietnamese[:, :-1]\n",
    "        targ_out = vietnamese[:, 1:]\n",
    "        return (tf.cast(english, tf.int64), tf.cast(targ_in, tf.int64)), tf.cast(targ_out, tf.int64)\n",
    "\n",
    "    return (\n",
    "        dataset.shuffle(10000)\n",
    "        .batch(BATCH_SIZE, drop_remainder=False)\n",
    "        .map(map_func, num_parallel_calls=tf.data.AUTOTUNE)  # Parallel mapping\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare the datasets\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = prepare_data_dynamic_parallel(train_data)\n",
    "val_dataset = prepare_data_dynamic_parallel(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example of how the shapes should look now\n",
    "for (en_batch, targ_in), targ_out in train_dataset.take(1):\n",
    "    print(\"English Batch Shape:\", en_batch.shape)\n",
    "    print(\"Vietnamese Input Batch Shape:\", targ_in.shape)\n",
    "    print(\"Vietnamese Output Batch Shape:\", targ_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check the dtype\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, MultiHeadAttention, LayerNormalization, Add, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "VOCAB_SIZE = 64000  \n",
    "UNITS = 256\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ENCODER LAYER\n",
    "\n",
    "encoder_input = tf.keras.Input(shape=(None,), dtype=tf.int64)\n",
    "embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=UNITS, mask_zero=False)(encoder_input)\n",
    "encoder_embedding_dropout = Dropout(0.2)(embedding) \n",
    "rnn_output = Bidirectional(LSTM(units=UNITS, return_sequences=True))(encoder_embedding_dropout)\n",
    "encoder_output = Dense(UNITS)(rnn_output)  \n",
    "\n",
    "# CROSS-ATTENTION LAYER\n",
    "\n",
    "decoder_input = tf.keras.Input(shape=(None,), dtype=tf.int64)\n",
    "decoder_embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=UNITS, mask_zero=False)(decoder_input)\n",
    "decoder_embedding_dropout = Dropout(0.2)(decoder_embedding)\n",
    "pre_attention_rnn = LSTM(units=UNITS, return_sequences=True, return_state=True)(decoder_embedding_dropout)\n",
    "attn_output = MultiHeadAttention(key_dim=UNITS, num_heads=4)(query=pre_attention_rnn[0], value=encoder_output)\n",
    "attn_output = Add()([pre_attention_rnn[0], attn_output])\n",
    "attn_output = LayerNormalization()(attn_output)\n",
    "\n",
    "# DECODER LAYER\n",
    "post_attention_rnn = LSTM(units=UNITS, return_sequences=True)(attn_output)\n",
    "logits = Dense(VOCAB_SIZE, activation='softmax')(post_attention_rnn)\n",
    "\n",
    "# FINAL MODEL\n",
    "model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def compile_and_train(model, epochs=40, steps_per_epoch=3200):\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',  \n",
    "        patience=4,          \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset.repeat(),\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=350,\n",
    "        callbacks=[early_stopping]  \n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "trained_translator, history = compile_and_train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_translator.save('/kaggle/working/best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = load_model('/kaggle/working/best_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def greedy_decode(input_sequence, model, tokenizer_target, max_length=50):\n",
    "\n",
    "    input_sequence = tf.constant([input_sequence], dtype=tf.int64)\n",
    "\n",
    "    start_token = tokenizer_vi.cls_token_id\n",
    "    end_token = tokenizer_vi.sep_token_id\n",
    "\n",
    "    target_sequence = [start_token]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        decoder_input = tf.constant([target_sequence], dtype=tf.int64)\n",
    "\n",
    "        predictions = model.predict([input_sequence, decoder_input], verbose=0)\n",
    "\n",
    "        next_token = tf.argmax(predictions[:, -1, :], axis=-1).numpy()[0]\n",
    "\n",
    "        target_sequence.append(next_token)\n",
    "\n",
    "        if next_token == end_token:\n",
    "            break\n",
    "\n",
    "    translated_sentence = tokenizer_target.decode(target_sequence[1:], skip_special_tokens=True)\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "en_sentence = \"I go to school\"\n",
    "\n",
    "input_tokens = tokenizer_en.encode(en_sentence, add_special_tokens=True)\n",
    "\n",
    "translated_sentence = greedy_decode(input_tokens, model, tokenizer_vi)\n",
    "\n",
    "print(\"Input Sentence:\", en_sentence)\n",
    "print(\"Translated Sentence:\", translated_sentence)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6279240,
     "sourceId": 10167921,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
